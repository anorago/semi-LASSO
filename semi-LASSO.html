<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.22">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nicolas Champagnat">
<meta name="author" content="Anne Gégout-Petit">
<meta name="author" content="Anouk Rago">
<meta name="dcterms.date" content="2023-01-02">
<meta name="keywords" content="regression, LASSO, prior knowledge, R, gene network inference">
<meta name="description" content="This document provides a template based on the quarto system for contributions to Computo. The github repository in itself provides a specific quarto extension useful for authors (and editors!).">

<title>Semi-Lasso: a weighted Lasso designed for the integration of known regressors in linear model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="semi-LASSO_files/libs/clipboard/clipboard.min.js"></script>
<script src="semi-LASSO_files/libs/quarto-html/quarto.js"></script>
<script src="semi-LASSO_files/libs/quarto-html/popper.min.js"></script>
<script src="semi-LASSO_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="semi-LASSO_files/libs/quarto-html/anchor.min.js"></script>
<link href="semi-LASSO_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="semi-LASSO_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="semi-LASSO_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="semi-LASSO_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="semi-LASSO_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="semi-LASSO_files/libs/quarto-contrib/pseudocode-2.4/pseudocode.min.js"></script>
<link href="semi-LASSO_files/libs/quarto-contrib/pseudocode-2.4/pseudocode.min.css" rel="stylesheet">
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #FFFFFF;
      }

      .quarto-title-block .quarto-title-banner {
        color: #FFFFFF;
background: #034E79;
      }
</style>
<meta name="quarto:status" content="draft">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Semi-Lasso: a weighted Lasso designed for the integration of known regressors in linear model">
<meta name="citation_keywords" content="regression,LASSO,prior knowledge,R,gene network inference">
<meta name="citation_author" content="Nicolas Champagnat">
<meta name="citation_author" content="Anne Gégout-Petit">
<meta name="citation_author" content="Anouk Rago">
<meta name="citation_publication_date" content="2023-01-02">
<meta name="citation_cover_date" content="2023-01-02">
<meta name="citation_year" content="2023">
<meta name="citation_online_date" content="2023-01-02">
<meta name="citation_fulltext_html_url" content="https://github.com/computorg/computo-quarto-extension">
<meta name="citation_doi" content="10.xxxx/xxx-xxx">
<meta name="citation_issn" content="2824-7795">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="Computo">
<meta name="citation_reference" content="citation_title=Regression shrinkage and selection via the lasso;,citation_author=Robert Tibshirani;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_issue=1;,citation_volume=58;,citation_journal_title=Journal of the Royal Statistical Society Series B: Statistical Methodology;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=The adaptive lasso and its oracle properties;,citation_author=Hui Zou;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_journal_title=Journal of the American statistical association;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Regularization paths for generalized linear models via coordinate descent;,citation_author=Jerome Friedman;,citation_author=Trevor Hastie;,citation_author=Rob Tibshirani;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_issue=1;,citation_volume=33;,citation_journal_title=Journal of statistical software;,citation_publisher=NIH Public Access;">
<meta name="citation_reference" content="citation_title=Regularization and variable selection via the elastic net;,citation_author=Hui Zou;,citation_author=Trevor Hastie;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_issue=2;,citation_volume=67;,citation_journal_title=Journal of the Royal Statistical Society Series B: Statistical Methodology;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=Model selection and estimation in regression with grouped variables;,citation_author=Ming Yuan;,citation_author=Yi Lin;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=1;,citation_volume=68;,citation_journal_title=Journal of the Royal Statistical Society Series B: Statistical Methodology;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=Regularization and variable selection via the elastic net;,citation_author=Hui Zou;,citation_author=Trevor Hastie;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_issue=2;,citation_volume=67;,citation_journal_title=Journal of the Royal Statistical Society Series B: Statistical Methodology;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=Least angle regression;,citation_author=Bradley Efron;,citation_author=Trevor Hastie;,citation_author=Iain Johnstone;,citation_author=Robert Tibshirani;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_issue=2;,citation_volume=32;,citation_journal_title=The Annals of Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Incorporating existing network information into gene network inference;,citation_author=Scott Christley;,citation_author=Qing Nie;,citation_author=Xiaohui Xie;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=8;,citation_volume=4;,citation_journal_title=PloS one;,citation_publisher=Public Library of Science San Francisco, USA;">
<meta name="citation_reference" content="citation_title=Robust data-driven incorporation of prior knowledge into the inference of dynamic regulatory networks;,citation_author=Alex Greenfield;,citation_author=Christoph Hafemeister;,citation_author=Richard Bonneau;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=8;,citation_volume=29;,citation_journal_title=Bioinformatics;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=Stability approach to regularization selection (stars) for high dimensional graphical models;,citation_author=Han Liu;,citation_author=Kathryn Roeder;,citation_author=Larry Wasserman;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=23;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Weighted-LASSO for structured network inference from time course data;,citation_author=Camille Charbonnier;,citation_author=Julien Chiquet;,citation_author=Christophe Ambroise;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_issue=1;,citation_volume=9;,citation_journal_title=Statistical applications in genetics and molecular biology;,citation_publisher=De Gruyter;">
<meta name="citation_reference" content="citation_title=Weighted lasso with data integration;,citation_author=Linn Cecilie Bergersen;,citation_author=Ingrid K Glad;,citation_author=Heidi Lyng;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=1;,citation_volume=10;,citation_journal_title=Statistical applications in genetics and molecular biology;,citation_publisher=De Gruyter;">
<meta name="citation_reference" content="citation_title=Functional association networks as priors for gene regulatory network inference;,citation_author=Matthew E Studham;,citation_author=Andreas Tjärnberg;,citation_author=Torbjörn EM Nordling;,citation_author=Sven Nelander;,citation_author=Erik LL Sonnhammer;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=12;,citation_volume=30;,citation_journal_title=Bioinformatics;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=Optimizing data integration improves gene regulatory network inference in arabidopsis thaliana;,citation_author=Océane Cassan;,citation_author=Charles Henri Lecellier;,citation_author=Antoine Martin;,citation_author=Laurent Brehelin;,citation_author=Sophie Lèbre;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
<meta name="citation_reference" content="citation_title=Impact de la dépendance dans les procédures de tests multiples en grande dimension;,citation_author=Chloé Friguet;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_dissertation_institution=Agrocampus-Ecole nationale supérieure d’agronomie de rennes;">
<meta name="citation_reference" content="citation_title=Gene regulatory network inference: An introductory survey;,citation_author=Vân Anh Huynh-Thu;,citation_author=Guido Sanguinetti;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_journal_title=Gene regulatory networks: Methods and protocols;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Overview and evaluation of recent methods for statistical inference of gene regulatory networks from time series data;,citation_author=Marco Grzegorczyk;,citation_author=Andrej Aderhold;,citation_author=Dirk Husmeier;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_journal_title=Gene Regulatory Networks: Methods and Protocols;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Integrative approaches for inference of genome-scale gene regulatory networks;,citation_author=Alireza Fotuhi Siahpirani;,citation_author=Deborah Chasman;,citation_author=Sushmita Roy;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_journal_title=Gene Regulatory Networks: Methods and Protocols;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Survival of the sparsest: Robust gene networks are parsimonious;,citation_author=Robert D Leclerc;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=1;,citation_volume=4;,citation_journal_title=Molecular systems biology;,citation_publisher=John Wiley &amp;amp;amp; Sons, Ltd Chichester, UK;">
<meta name="citation_reference" content="citation_title=DIP: The database of interacting proteins;,citation_author=Ioannis Xenarios;,citation_author=Danny W Rice;,citation_author=Lukasz Salwinski;,citation_author=Marisa K Baron;,citation_author=Edward M Marcotte;,citation_author=David Eisenberg;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_issue=1;,citation_volume=28;,citation_journal_title=Nucleic acids research;,citation_publisher=Oxford University Press;">
</head>

<body><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Semi-Lasso: a weighted Lasso designed for the integration of known regressors in linear model</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
            <div>
        <div class="description">
          <p>This document provides a template based on the quarto system for contributions to Computo. The github repository in itself provides a specific quarto extension useful for authors (and editors!).</p>
        </div>
      </div>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliation</div>
          
          <div class="quarto-title-meta-contents">
        Nicolas Champagnat 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Université de Lorraine, CNRS, Inria, IECL, UMR 7502
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        Anne Gégout-Petit 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Université de Lorraine, CNRS, Inria, IECL, UMR 7502
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        Anouk Rago 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Université de Lorraine, CNRS, Inria, IECL, UMR 7502
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 2, 2023</p>
      </div>
    </div>
                                    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">March 4, 2024</p>
      </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.xxxx/xxx-xxx">10.xxxx/xxx-xxx</a>
        </p>
      </div>
    </div>
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">regression, LASSO, prior knowledge, R, gene network inference</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <p class="date">draft</p>
                  </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>To encode prior information in a regression problem, statisticians may use a weighted LASSO, in which variables can have different weights in the penalization. In this paper, we propose an alternative method called semi-LASSO, which solves a specific case of weighted LASSO designed for the integration of known regressors in linear model. The optimization procedure is divided in two steps: the first one is an ordinary least squares method and the second one a classic LASSO procedure in lower dimension. Numerical experiments are performed on synthetic data to compare the performances of this new method with the usual weighted LASSO implemented in <code>glmnet</code>. The results show an improvement of the sensitivity, the variable selection and the prediction capability when using semi-LASSO.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#LASSO" id="toc-LASSO" class="nav-link" data-scroll-target="#LASSO"><span class="header-section-number">2</span> LASSO and weighted LASSO</a></li>
  <li><a href="#semilasso" id="toc-semilasso" class="nav-link" data-scroll-target="#semilasso"><span class="header-section-number">3</span> Semi-LASSO, a specific weighted LASSO with a priori knowledge</a>
  <ul class="collapse">
  <li><a href="#adding-prior-information-into-the-model" id="toc-adding-prior-information-into-the-model" class="nav-link" data-scroll-target="#adding-prior-information-into-the-model"><span class="header-section-number">3.1</span> Adding prior information into the model</a></li>
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation"><span class="header-section-number">3.2</span> Mathematical formulation</a></li>
  </ul></li>
  <li><a href="#simulations" id="toc-simulations" class="nav-link" data-scroll-target="#simulations"><span class="header-section-number">4</span> Numerical experiments</a>
  <ul class="collapse">
  <li><a href="#data-simu" id="toc-data-simu" class="nav-link" data-scroll-target="#data-simu"><span class="header-section-number">4.1</span> Data simulation</a></li>
  <li><a href="#introduction-of-a-priori-knowledge" id="toc-introduction-of-a-priori-knowledge" class="nav-link" data-scroll-target="#introduction-of-a-priori-knowledge"><span class="header-section-number">4.2</span> Introduction of a priori knowledge</a></li>
  <li><a href="#estimation-of-the-parameters" id="toc-estimation-of-the-parameters" class="nav-link" data-scroll-target="#estimation-of-the-parameters"><span class="header-section-number">4.3</span> Estimation of the parameters</a></li>
  <li><a href="#criteria" id="toc-criteria" class="nav-link" data-scroll-target="#criteria"><span class="header-section-number">4.4</span> Criteria used to compare the methods</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">5</span> Results</a>
  <ul class="collapse">
  <li><a href="#speci-sensi" id="toc-speci-sensi" class="nav-link" data-scroll-target="#speci-sensi"><span class="header-section-number">5.1</span> Sensitivity and specificity</a></li>
  <li><a href="#rmse" id="toc-rmse" class="nav-link" data-scroll-target="#rmse"><span class="header-section-number">5.2</span> RMSE</a></li>
  <li><a href="#parameters-estimation" id="toc-parameters-estimation" class="nav-link" data-scroll-target="#parameters-estimation"><span class="header-section-number">5.3</span> Parameters estimation</a></li>
  <li><a href="#other-tests-and-some-possible-extensions" id="toc-other-tests-and-some-possible-extensions" class="nav-link" data-scroll-target="#other-tests-and-some-possible-extensions"><span class="header-section-number">5.4</span> Other tests and some possible extensions</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6</span> Conclusion</a></li>
  <li><a href="#acknowlegments" id="toc-acknowlegments" class="nav-link" data-scroll-target="#acknowlegments">Acknowlegments</a></li>
  <li><a href="#session-information" id="toc-session-information" class="nav-link" data-scroll-target="#session-information">Session information</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="semi-LASSO.pdf"><i class="bi bi-file-pdf"></i>PDF (computo)</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The LASSO <span class="citation" data-cites="tibshirani1996regression">(<a href="#ref-tibshirani1996regression" role="doc-biblioref">Tibshirani 1996</a>)</span> is a widely used technique when it comes to perform both estimation of parameters and variable selection for a linear model. The penalty on the <span class="math inline">l_1</span> norm allows indeed to shrink some coefficients to <span class="math inline">0</span>. Moreover, it allows the user to handle cases where the number of variables <span class="math inline">p</span> is greater than the number of observations <span class="math inline">n</span> and thus is particularly convenient when one has to deal with large datasets like genomic data. This is why a lot of methods using LASSO and its derivative, like the adaptive LASSO <span class="citation" data-cites="zou2006adaptive">(<a href="#ref-zou2006adaptive" role="doc-biblioref">Zou 2006</a>)</span> or group LASSO <span class="citation" data-cites="yuan2006model">(<a href="#ref-yuan2006model" role="doc-biblioref">Yuan and Lin 2006</a>)</span> have been developed in particular to infer gene regulatory networks <span class="citation" data-cites="siahpirani2019integrative grzegorczyk2019overview">(<a href="#ref-siahpirani2019integrative" role="doc-biblioref">Siahpirani, Chasman, and Roy 2019</a>; <a href="#ref-grzegorczyk2019overview" role="doc-biblioref">Grzegorczyk, Aderhold, and Husmeier 2019</a>)</span>, which depict the relationships between genes. In these references, a regression model for each gene <span class="math inline">j \in [1,p]</span> is built, giving for each gene a list of regressors linked to it in the network. But inferring this network using only transcriptomic data can be a delicate task due to the massive amount of genes (<span class="math inline">p\approx 20 000</span>) present in the data compared to the small number of samples <span class="math inline">n</span>: the LASSO is then a good tool to select the most relevant regressors among the genes.</p>
<p>In some cases, some prior biological information can be available and therefore be integrated in the model to improve the results. For example, protein-protein interactions are experimentaly tested and the results are publicly available on the online databases of interacting proteins <span class="citation" data-cites="xenarios2000dip">(<a href="#ref-xenarios2000dip" role="doc-biblioref">Xenarios et al. 2000</a>)</span>. When using a LASSO, one way to perform prior information integration is to specify different penalty strengths for each gene/variable during the estimation of parameters, which is referred as the weighted LASSO and has been applied in various references. <span class="citation" data-cites="bergersen2011weighted">Bergersen, Glad, and Lyng (<a href="#ref-bergersen2011weighted" role="doc-biblioref">2011</a>)</span> use two different weights to add prior knowledge in their model : the first one is based on the correlation between gene expression and gene copy numbers, the second one relies on the association of gene copy number with survival. A weighted graphical LASSO is performed by <span class="citation" data-cites="charbonnier2010weighted">Charbonnier, Chiquet, and Ambroise (<a href="#ref-charbonnier2010weighted" role="doc-biblioref">2010</a>)</span> where prior information on the topology of the network is used. <span class="citation" data-cites="greenfield2013robust">Greenfield, Hafemeister, and Bonneau (<a href="#ref-greenfield2013robust" role="doc-biblioref">2013</a>)</span> rely on the elastic-net algorithm <span class="citation" data-cites="zou2005regularization">(<a href="#ref-zou2005regularization" role="doc-biblioref">Zou and Hastie 2005</a>)</span>, and modifies the <span class="math inline">l_1</span> norm to add prior knowledge on genes interactions. Moreover, it has been shown that optimizing the amount of prior knowledge included into the model gives significantly better results <span class="citation" data-cites="cassan2023optimizing">(<a href="#ref-cassan2023optimizing" role="doc-biblioref">Cassan et al. 2023</a>)</span>. From a larger perspective, specifying different penalty weights can be useful in any regression problem where prior information on the potential regressors is available. The <code>R</code> package <code>glmnet</code> <span class="citation" data-cites="friedman2010regularization">(<a href="#ref-friedman2010regularization" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2010</a>)</span> allows the user to do it, by tuning the <code>penalty.factor</code> parameter.</p>
<p>In this paper, we focus on a penalized linear regression model with prior information. We assume that this prior knowledge takes the form of a certainty that some potential regressors must belong to the model. In this particular case, their penalty strength is set to <span class="math inline">0</span> as we want to be sure to include them in the model. Concerning the other potential regressors for which we have no information, we assume that their penalty strength is the same for all. The number of known regressors should not exceed the number of observations <span class="math inline">n</span>, otherwise the method we propose would be inapplicable. As opposed to the procedure used in <code>glmnet</code>, which consists in optimizing the objective function directly with a cyclical coordinate descent method, we propose to first transform the problem to divide the optimization procedure in two steps, one which is an ordinary least squares method and the second one being a classic LASSO procedure in lower dimension. We will see that this procedure actually improves the parameters estimation, the true positive variables selection and reduces the error of prediction.<br>
The paper is organized as follows. In Section <a href="#LASSO">2</a>, we introduce the LASSO and weighted LASSO, and explain how to deal with the <code>glmnet</code> package. Section <a href="#semilasso">3</a> presents our method called semi-LASSO. Finally, we compare the performance of <code>glmnet</code> and semi-LASSO on simulated datasets in Section <a href="#simulations">4</a> and discuss the results in Section <a href="#results">5</a>.</p>
</section>
<section id="LASSO" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> LASSO and weighted LASSO</h1>
<p>We suppose that we have a set of <span class="math inline">p</span> explicative variables <span class="math inline">(X_1,\dots,X_p) \in \mathbb{R}^p</span> and a response variable <span class="math inline">Y \in \mathbb{R}</span> such that:</p>
<p><span id="eq-regression"><span class="math display">
Y = \beta_0 + \sum_{j=1}^p \beta_jX_j  + \epsilon \,,
\tag{1}</span></span> with <span class="math inline">\epsilon \sim \mathcal{N}(0,\sigma^2)</span> a centered noise and <span class="math inline">\boldsymbol \beta = (\beta_0,\beta_1,...,\beta_p) \in \mathbb{R}^{p+1}</span> the coefficients of the model, potentially equal to zero. We also assume having <span class="math inline">n</span> realizations of <span class="math inline">X_1,\dots,X_p</span> and <span class="math inline">Y</span>: we note <span class="math inline">\mathbf x = (x_{ij})</span> the <span class="math inline">n \times (p+1)</span> matrix of observations, the first column being filled with ones, and <span class="math inline">\mathbf y \in \mathbb{R}^n</span> the observations of <span class="math inline">Y</span>. Then the penalized LASSO regression is:</p>
<p><span id="eq-lassofunction"><span class="math display">
\min_{(\beta_0,\beta_1,\dots,\beta_p)} \quad \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2 + \lambda \text{P}(\boldsymbol\beta) \,,
\tag{2}</span></span></p>
<p>with <span class="math inline">\lambda</span> a positive regularization parameter and <span class="math inline">\text{P}(\boldsymbol\beta)</span> the LASSO penalty defined by: <span id="eq-penalty"><span class="math display">
\text{P}(\boldsymbol\beta) =\sum_{j=1}^p|\beta_j| \,.
\tag{3}</span></span> The bigger <span class="math inline">\lambda</span> is, the more the coefficients <span class="math inline">\boldsymbol \beta</span> are penalized, which leads to more <span class="math inline">0</span> coefficients and a sparser model. To solve <a href="#eq-lassofunction" class="quarto-xref">Equation&nbsp;2</a>, two methods can be used. The first one, called the LARS algorithm <span class="citation" data-cites="efron2004least">(<a href="#ref-efron2004least" role="doc-biblioref">Efron et al. 2004</a>)</span> and implemented in the <code>R</code> package <code>lars</code>, relies on the correlation between the explicative variables and <span class="math inline">Y</span>, and is not discussed here. The second one is a coordinate gradient descent directly applied on the objective function, as done in the <code>R</code> package <code>glmnet</code> <span class="citation" data-cites="friedman2010regularization">(<a href="#ref-friedman2010regularization" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2010</a>)</span>. In <a href="#eq-penalty" class="quarto-xref">Equation&nbsp;3</a>, all <span class="math inline">\beta_j</span> coefficients are treated the same way and are equally penalized, as the data is beforehand normalized before solving the optimization problem in <code>glmnet</code>. A possible extension for the LASSO is to add a different weight for each variable. The objective lying behind these weights can either be:</p>
<ul>
<li>to reduce the conflict between optimal prediction and consistent variable selection, thus achieving some theoretical properties. This is the adaptive LASSO, introduced by <span class="citation" data-cites="zou2006adaptive">Zou (<a href="#ref-zou2006adaptive" role="doc-biblioref">2006</a>)</span>. In this case, the weights are first determined and linked to the data, then iteratively refined to reach some oracle properties.</li>
<li>to add some prior information on the potential regressors of the model <span class="citation" data-cites="bergersen2011weighted charbonnier2010weighted greenfield2013robust">(<a href="#ref-bergersen2011weighted" role="doc-biblioref">Bergersen, Glad, and Lyng 2011</a>; <a href="#ref-charbonnier2010weighted" role="doc-biblioref">Charbonnier, Chiquet, and Ambroise 2010</a>; <a href="#ref-greenfield2013robust" role="doc-biblioref">Greenfield, Hafemeister, and Bonneau 2013</a>)</span>, as detailed in Section <a href="#introduction">1</a>. This method is called the weighted LASSO. This variant of the LASSO can also resolve the issue raised by variables with similar profiles: only one of them is generally included into the model, not always the most relevant one. By including prior knowledge and so different weights in the objective function, the optimization step leads to a more relevant variables selection.</li>
</ul>
<p>In these cases, the penalty function becomes:</p>
<p><span id="eq-weighted-penalty"><span class="math display">
\text{P}_{\boldsymbol w}\,(\boldsymbol\beta) =\sum_{j=1}^p w_j\,|\beta_j| \,.
\tag{4}</span></span> where <span class="math inline">\boldsymbol w = (w_1,\dots,w_p) \in (\mathbb{R}^+)^p</span> is the weights vector. A coefficient with a small weight will be less penalized than a coefficient with a big one, and is more likely to be included into the final model. This new problem can be solved with the same gradient descent method of <code>glmnet</code> used to solve <a href="#eq-lassofunction" class="quarto-xref">Equation&nbsp;2</a>. When using this <code>R</code> package, it is possible to specify particular weights with the <code>penalty.factor</code> parameter.</p>
</section>
<section id="semilasso" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Semi-LASSO, a specific weighted LASSO with a priori knowledge</h1>
<section id="adding-prior-information-into-the-model" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="adding-prior-information-into-the-model"><span class="header-section-number">3.1</span> Adding prior information into the model</h2>
<p>Linear regression models with penalization are very used in the field of gene network inference <span class="citation" data-cites="siahpirani2019integrative grzegorczyk2019overview huynh2019gene">(<a href="#ref-siahpirani2019integrative" role="doc-biblioref">Siahpirani, Chasman, and Roy 2019</a>; <a href="#ref-grzegorczyk2019overview" role="doc-biblioref">Grzegorczyk, Aderhold, and Husmeier 2019</a>; <a href="#ref-huynh2019gene" role="doc-biblioref">Huynh-Thu and Sanguinetti 2019</a>)</span>. First because they can deal with big data especially when <span class="math inline">n\ll p</span>, then because they can select only a few interesting genes as regressors. Biologists are indeed aware that gene regulatory networks are sparse <span class="citation" data-cites="leclerc2008survival">(<a href="#ref-leclerc2008survival" role="doc-biblioref">Leclerc 2008</a>)</span>, which makes of the LASSO an interesting method as it offers the possibility to perform variable selection among thousands of genes. But today, a lot of biological information is available concerning the links existing between genes. This information can be added in the model by using a weighted LASSO as described in Section <a href="#LASSO">2</a>, with carefully chosen weights.<br>
For this purpose, we focus in this paper on a specific case of weighted LASSO, where the weights can be either <span class="math inline">0</span>, which means no penalization, or <span class="math inline">1</span>, as in the usual LASSO. Concretely, a weight equals to <span class="math inline">0</span> means that the variable associated to this weight will always be included in the model: the associated regression coefficient <span class="math inline">\beta_j</span> will generically be non-zero. It corresponds to the variable for which we are certain that it has a relationship with <span class="math inline">Y</span>, for example a known protein-protein interaction.<br>
The variables <span class="math inline">\mathbf X</span> are then divided into two groups:</p>
<ul>
<li><span class="math inline">G_K</span> -K for Known-, the set of variables for which we are certain they are part of the model, as we have prior knowledge on their relationship with <span class="math inline">Y</span>. The method does not penalize these variables and their weight will be set to <span class="math inline">0</span>.<br>
</li>
<li><span class="math inline">G_{U}</span> -U for Unknown-, the set of variables for which we have no prior information on their relationship with <span class="math inline">Y</span>. Their weights will be set to <span class="math inline">1</span>, each of them being penalized the same way by the parameter <span class="math inline">\lambda</span>.</li>
</ul>
<p>As it seems reasonable to assume the presence of an intercept in the majority of real data modeling, the variable corresponding to the parameter <span class="math inline">\beta_0</span> in <a href="#eq-regression" class="quarto-xref">Equation&nbsp;1</a> is always set in <span class="math inline">G_K</span>. The sum of cardinals of the two groups is <span class="math inline">G_K</span> and <span class="math inline">G_U</span> is <span class="math inline">p+1</span> and we also assume in what follows that the number of variables in <span class="math inline">G_K</span> never exceeds <span class="math inline">n</span>, otherwise the method we propose would be inapplicable.</p>
</section>
<section id="mathematical-formulation" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="mathematical-formulation"><span class="header-section-number">3.2</span> Mathematical formulation</h2>
<p>Using the notations above and those of Section <a href="#LASSO">2</a>, the problem we want to find the argmin in the following minimization problem:</p>
<p><span id="eq-semilasso"><span class="math display">
\min_{(\beta_0,\beta_1,\dots,\beta_p)} \quad \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2  + \lambda \text{P}_{\boldsymbol 1}(\boldsymbol\beta)\,,
\tag{5}</span></span> with : <span class="math display">
\text{P}_{\boldsymbol 1}\,(\boldsymbol\beta)  = \sum_{j=1}^p\,\mathbb{1}_{X_j \in G_U}|\beta_j|= \sum_{j,\,X_j \in G_U}\,|\beta_j| \,. \\
</span> We note <span class="math inline">\boldsymbol \beta_K</span> (respectively <span class="math inline">\boldsymbol \beta_U</span>), the vector <span class="math inline">\boldsymbol \beta_K = (\beta_j, \,X_j \in G_K)</span> (respectively <span class="math inline">\boldsymbol \beta_U = (\beta_j, \,X_j \in G_U)</span>). In a similar way, we denote by <span class="math inline">\mathbf x_K</span> (respectively <span class="math inline">\mathbf x_U</span>) the matrix containing the <span class="math inline">n</span> observations of the variables in <span class="math inline">G_K</span> (respectively <span class="math inline">G_U</span>). <a href="#eq-semilasso" class="quarto-xref">Equation&nbsp;5</a> can be rewritten as:</p>
<p><span class="math display">
\min_{\boldsymbol \beta_U} \big( \min_{\boldsymbol \beta_K} \,\big( \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2 \big)\,+ \lambda \sum_{j,\,X_j \in G_U}\,|\beta_j| \big) \,.
</span></p>
<p>The minimum in <span class="math inline">\boldsymbol \beta_K</span> takes the form of : <span id="eq-mini"><span class="math display">
\begin{aligned}
\min_{\boldsymbol \beta_K} \, \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2  
&amp; = \min_{\boldsymbol \beta_K} \, \frac{1}{2n}\sum_{i=1}^n(y_i-\beta_0- \sum_{j,\,X_j \in G_U} x_{ij}\beta_j -\sum_{j,\,X_j \in G_K} x_{ij}\beta_j)^2\\
&amp;= \min_{\boldsymbol \beta_K} \, \frac{1}{2n}\sum_{i=1}^n(z_i-\beta_0 -\sum_{j,\,X_j \in G_K} x_{ij}\beta_j)^2 \,,
\end{aligned}
\tag{6}</span></span> with <span class="math inline">\mathbf z = (z_i)_{1\leq i \leq n} = (y_i-\sum\limits_{j \in G_{U}}x_{ij}\beta_j)_{1\leq i \leq n} = \mathbf y-\mathbf x_{U}\boldsymbol\beta_{U}</span> . The minimum above is reached when <span class="math inline">\boldsymbol \beta_K</span> is the ordinary least squares estimator, under the assumption that <span class="math inline">\mathbf x_K^t\mathbf x_K</span> is invertible, which implies in particular that the cardinal of <span class="math inline">G_K</span> is lower than <span class="math inline">n</span>. We have:</p>
<p><span id="eq-betaK"><span class="math display">
\text{}\widehat{\boldsymbol\beta_K}=\widehat{\boldsymbol\beta_K}(\boldsymbol\beta_U) = (\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t\mathbf z \,.
\tag{7}</span></span></p>
<p>Using this expression for <span class="math inline">\widehat{\boldsymbol\beta_K}</span>, <a href="#eq-mini" class="quarto-xref">Equation&nbsp;6</a> becomes:</p>
<p><span class="math display">
\begin{aligned}
\min_{\boldsymbol \beta_K} \, \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2  &amp;=\frac{1}{2n}|| \mathbf y-\mathbf x_{U}\boldsymbol\beta_U-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t(\mathbf y-\mathbf x_{U}\boldsymbol\beta_U)  ||_2^2 \\
    &amp;= \frac{1}{2n}||  (I_n-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t)\mathbf y-(\mathbf x_{U}-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t\mathbf x_{U})\boldsymbol\beta_U||^2_2 \\
    &amp;= \frac{1}{2n}|| \mathbf u -  \mathbf v \boldsymbol \beta_U||^2_2 \,,
\end{aligned}
</span> with <span class="math inline">\mathbf u = (I_n-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t)\mathbf y</span> and <span class="math inline">\mathbf v = \mathbf x_{U}-\mathbf x_K(\mathbf x_K^T\mathbf x_K)^{-1}\mathbf x_K^t\mathbf x_{U}</span>. <a href="#eq-semilasso" class="quarto-xref">Equation&nbsp;5</a> becomes :</p>
<p><span id="eq-rewritten-semilasso"><span class="math display">
\min_{\boldsymbol \beta_U} \big( \frac{1}{2n}|| \mathbf u - \mathbf v \boldsymbol \beta_U||^2_2 + \lambda \sum_{j,\,X_j \in G_U}\,|\beta_j| \big) \,.
\tag{8}</span></span></p>
<p>Hence the minimization problem above corresponds to the classic objective function of the LASSO, and only depends on <span class="math inline">\boldsymbol \beta_U</span>. The solution of <a href="#eq-rewritten-semilasso" class="quarto-xref">Equation&nbsp;8</a> can thus be found numerically, using <code>glmnet</code> for example. Once the solution <span class="math inline">\widehat{\boldsymbol \beta_U}</span> is computed, we can obtain <span class="math inline">\widehat{\boldsymbol\beta_K}</span> by injecting the obtained value for <span class="math inline">\widehat{\boldsymbol \beta_U}</span> in <a href="#eq-betaK" class="quarto-xref">Equation&nbsp;7</a>. The pseudocode corresponding to this procedure of estimation, called semi-LASSO, is given below in <a href="#alg-semilasso">Algorithm 1</a>.</p>
<p>An alternative solution would be to use <code>glmnet</code> directly, by specifying <span class="math inline">0</span> and <span class="math inline">1</span> weights in the <code>penalty.factor</code> argument: this procedure will be refered as 0-1 weighted LASSO in what follows.</p>
<p>The semi-LASSO method can be more efficient than the 0-1 weighted LASSO for the following reasons. By separating the variables in two distinct groups before numerically optimizing the objective function <a href="#eq-semilasso" class="quarto-xref">Equation&nbsp;5</a>, we find ourselves solving two different minimization problems. The first one is an ordinary least squares regression, which has a well-known analytical solution, and the second one is a classic LASSO regression. This second problem will be solved numerically, but in a space of smaller dimension (less variables) compared to the 0-1 weighted LASSO. We will see in the next sections that our semi-LASSO method actually improves the parameter estimation, the sensititvity and reduces the error of prediction.</p>
<div id="alg-semilasso" class="pseudocode-container" data-comment-delimiter="//" data-pseudocode-index="1" data-no-end="false" data-line-number-punc=":" data-line-number="true" data-alg-title="Algorithm" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{Semi-LASSO} \begin{algorithmic} \State \textbf{Inputs}: \\ $\quad$ $G_K$ and $G_U$ the groups of variables \\ $\quad$ $\mathbf x \in \mathcal{M}_{n \times p}(\mathbb{R})$ the data matrix \\ $\quad$ $\mathbf y \in \mathbb{R}^n$ the vector of observed values for $Y$ \\ \State \textbf{Output:} \\ $\quad$ $\boldsymbol{\beta}$ the vector of coefficients \\ \If{$G_K = \emptyset$} \Comment{No prior information is known} \State $\boldsymbol \beta$ obtained with classical LASSO \\ \ElsIf{$G_U = \emptyset$ \And $|G_K| \leq n$ } \Comment{All regressors are known} \State $\boldsymbol \beta$ obtained with classical regression \\ \Else \State compute $\mathbf x_K$ and $\mathbf x_U$ from $\mathbf x$ \State add a column of $1$ in $\mathbf x_K$ for the intercept estimation \State compute $\mathbf u = (I_n-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t)\mathbf y$ and $ \mathbf v = \mathbf x_{U}-\mathbf x_K(\mathbf x_K^T\mathbf x_K)^{-1}\mathbf x_K^t\mathbf x_{U}$ \State solve $\min_{\boldsymbol \beta_U} \big( \frac{1}{2n}|| \mathbf u - \boldsymbol \beta_U\mathbf v||^2_2,+ \lambda \sum_{j,\,X_j \in G_U}\,|\beta_j| \big)$ with glmnet to obtain $\widehat{\boldsymbol\beta_U}$ \State deduce $\widehat{\boldsymbol\beta_K}$ \EndIf \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
</section>
<section id="simulations" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Numerical experiments</h1>
<p>To compare our method of parameters estimation with the one of 0-1 weighted LASSO, we perform numerical simulations. We focus on simulations where the number of observations <span class="math inline">n=100</span> and the number of covariates <span class="math inline">p =500</span>. We also choose to run simulations where the explanatory variables <span class="math inline">(X_1,\dots,X_p)</span> are correlated, to put our algorithm in a more challenging situation. Finally, only <span class="math inline">p_u = 40</span> variables are actually used to build the model which means only <span class="math inline">40</span> coordinates of <span class="math inline">\boldsymbol \beta \in \mathbb{R}^p</span> are really used to construct <span class="math inline">Y</span>.</p>
<section id="data-simu" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="data-simu"><span class="header-section-number">4.1</span> Data simulation</h2>
<p>To simulate correlated data in a realistic way, we rely on the work by <span class="citation" data-cites="friguet2010impact">Friguet (<a href="#ref-friguet2010impact" role="doc-biblioref">2010</a>)</span>. We first divide the <span class="math inline">p</span> variables in <span class="math inline">H</span> independent groups. For each group <span class="math inline">h</span>, a covariance matrix is computed using gaussian latent variables to determine correlations between variables, and a dataset of size <span class="math inline">n \times \frac{p}{H}</span> is sampled from a multivariate normal distribution. The data generated for each group <span class="math inline">h \in [1,H]</span> are then merged together in a single dataset. The structure of the global covariance matrix of the data is thus block diagonal. An excerpt of this matrix is presented <a href="#fig-cor" class="quarto-xref">Figure&nbsp;1</a> as an example. The code to generate the data, based on page <span class="math inline">123</span> of <span class="citation" data-cites="friguet2010impact">Friguet (<a href="#ref-friguet2010impact" role="doc-biblioref">2010</a>)</span>, is given below, the parameter <span class="math inline">H</span> being set to <span class="math inline">5</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(corrplot,<span class="at">quietly =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>corrplot 0.92 loaded</code></pre>
</div>
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">500</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>H <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>constant <span class="ot">=</span> <span class="dv">10</span> <span class="co"># intercept</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>q <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">4</span>,H) <span class="co"># number of latent variables in each group</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>strength <span class="ot">=</span> <span class="dv">7</span><span class="sc">/</span><span class="dv">10</span> <span class="co"># level of the correlation between variables of the same group</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">0</span>, n<span class="sc">*</span>p), <span class="at">ncol =</span> p, <span class="at">nrow =</span> n)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (h <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>H){</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  B_k <span class="ot">=</span> strength<span class="sc">*</span><span class="fu">rmvnorm</span>(p<span class="sc">/</span>H, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>, q[h]), <span class="at">sigma =</span> <span class="fu">diag</span>(q[h]))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  Sigma_k <span class="ot">=</span> <span class="fu">apply</span>(B_k<span class="sc">^</span><span class="dv">2</span>, <span class="dv">1</span>, sum) <span class="sc">+</span> <span class="fu">rep</span>(<span class="dv">1</span>, p<span class="sc">/</span>H)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  B_k <span class="ot">=</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(Sigma_k))<span class="sc">%*%</span>B_k</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  Psi_k <span class="ot">=</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span>Sigma_k)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># matrix of var-covar</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  Var_k <span class="ot">=</span> B_k <span class="sc">%*%</span> <span class="fu">t</span>(B_k) <span class="sc">+</span> Psi_k</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># data generation</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  X_k <span class="ot">=</span> <span class="fu">rmvnorm</span> (n, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">/</span>H), <span class="at">sigma =</span> Var_k) <span class="co"># mean 0</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  data[,((h<span class="dv">-1</span>)<span class="sc">*</span>p<span class="sc">/</span>H<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(h<span class="sc">*</span>p<span class="sc">/</span>H)] <span class="ot">=</span> X_k </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">as.data.frame</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"data/V_k_same.RData"</span>) <span class="co"># final matrix of var covar</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>Var_covar <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">*</span>p),<span class="at">ncol=</span>p,<span class="at">nrow =</span> p)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (h <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  Var_covar[((h<span class="dv">-1</span>)<span class="sc">*</span>p<span class="sc">/</span>H<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(h<span class="sc">*</span>p<span class="sc">/</span>H),((h<span class="dv">-1</span>)<span class="sc">*</span>p<span class="sc">/</span>H<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(h<span class="sc">*</span>p<span class="sc">/</span>H)] <span class="ot">=</span> Var_list[[h]]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">corrplot</span>(Var_covar[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,<span class="dv">101</span><span class="sc">:</span><span class="dv">120</span>,<span class="dv">201</span><span class="sc">:</span><span class="dv">220</span>,<span class="dv">301</span><span class="sc">:</span><span class="dv">320</span>,<span class="dv">401</span><span class="sc">:</span><span class="dv">420</span>),<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,<span class="dv">101</span><span class="sc">:</span><span class="dv">120</span>,<span class="dv">201</span><span class="sc">:</span><span class="dv">220</span>,<span class="dv">301</span><span class="sc">:</span><span class="dv">320</span>,<span class="dv">401</span><span class="sc">:</span><span class="dv">420</span>)],<span class="at">tl.pos =</span> <span class="st">'n'</span>,<span class="at">addgrid.col=</span><span class="cn">NA</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a> <span class="co">#c = corrplot(cor(data[,c(1:20,101:120,201:220,301:320,401:420)]),tl.pos = 'n',addgrid.col=NA) empiric matrix</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-cor" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="semi-LASSO_files/figure-html/fig-cor-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Correlation matrix of the variables of the model. Only the first <span class="math inline">20</span> variables of each group are plotted.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Once we got the data matrix <span class="math inline">\boldsymbol x</span>, we compute <span class="math inline">\mathbf y</span> using the <span class="math inline">8</span> first variables of each group. The coefficients <span class="math inline">\beta_j</span> which are not <span class="math inline">0</span> go from <span class="math inline">1</span> to <span class="math inline">8</span> in each group. Our final model for the numerical simulations is thus: <span class="math display">
y = \beta_0 + \sum_{j=1}^8 \beta_jx_j  +  \sum_{j=101}^{108} \beta_jx_j +  \sum_{j=201}^{208} \beta_jx_j + \sum_{j=301}^{308} \beta_jx_j + \sum_{j=401}^{408} \beta_jx_j + \epsilon \,,
</span> with <span class="math inline">\beta_1 = \beta_{101} = \dots = \beta_{401} = 1, \quad \dots \quad,\beta_8 = \beta_{108} = \dots = \beta_{408} = 8</span> and <span class="math inline">\epsilon \sim \mathcal{N}(0,4)</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(expm,<span class="at">quietly =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'expm'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:Matrix':

    expm</code></pre>
</div>
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>standard_d <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>,<span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">92</span>)),<span class="dv">5</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">as.matrix</span>(data)<span class="sc">%*%</span>beta <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> standard_d) <span class="sc">+</span> <span class="fu">rep</span>(constant, n)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Y) <span class="ot">=</span> <span class="st">"Y"</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">cbind</span>(data,Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="introduction-of-a-priori-knowledge" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="introduction-of-a-priori-knowledge"><span class="header-section-number">4.2</span> Introduction of a priori knowledge</h2>
<p>To see how the semi-LASSO algorithm performs with different levels of prior knowledge, we progressively add variables from the <span class="math inline">G_U</span> group to the <span class="math inline">G_K</span> group. <span class="math inline">11</span> scenarii are tested, starting with with <span class="math inline">G_K = \emptyset</span> where no regressor is known. We then add <span class="math inline">4</span> variables (corresponding to <span class="math inline">10\%</span> of true regressors) in <span class="math inline">G_K</span> several times to finally reach <span class="math inline">G_K = (X_{j_1},\dots,X_{j_{p_u}})</span> where <span class="math inline">X_{j_1},\dots,X_{j_{p_u}}</span> are the <span class="math inline">p_u</span> variables used to build the model <span class="math inline">(j_1 = 1,\dots,j_8=8,j_9=101,...,j_{40}=408)</span>, as explained in <a href="#tbl-scenario" class="quarto-xref">Table&nbsp;1</a>. One variable of each group <span class="math inline">h \in [1,H-1]</span> is added in scenarii <span class="math inline">s_1</span> to <span class="math inline">s_8</span>, then four variables of the last group <span class="math inline">h=H</span> are added in <span class="math inline">s_9</span> and <span class="math inline">s_{10}</span>. In scenario <span class="math inline">s_{10}</span>, all <span class="math inline">p_u</span> variables have been added to <span class="math inline">G_K</span>.</p>
<div id="tbl-scenario" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-scenario-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Description of each scenario to estimate the <span class="math inline">\boldsymbol \beta</span> coefficients
</figcaption>
<div aria-describedby="tbl-scenario-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 33%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Scenario</th>
<th style="text-align: center;">% of prior knowledge</th>
<th style="text-align: center;">Variables in <span class="math inline">G_K</span></th>
<th style="text-align: center;">Variables in <span class="math inline">G_U</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">s_0</span></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">500</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">s_1</span></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">496</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">s_2</span></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">492</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">s_3</span></td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">488</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">s_4</span></td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">484</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">s_5</span></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">480</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">s_6</span></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">476</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">s_7</span></td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">472</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">s_8</span></td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">468</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">s_9</span></td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">464</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">s_{10}</span></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">460</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Construction of the vector of prior knowledge used to progressively add variables in G_K</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>prior_know <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (h <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(H<span class="dv">-1</span>)){</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># for the first 4 groups, the first 8 variables will be progressively added to G_K </span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (corresponding to scenarii s_1 to s_8)</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  prior_know_h <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">10</span><span class="sc">*</span><span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>, <span class="fu">rep</span>(<span class="st">"not used"</span>,<span class="dv">92</span>)) </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># for the variables not used in the model, prior_know is set to `not used`. </span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># It means they will never be included as prior knowledge in the model</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  prior_know <span class="ot">=</span> <span class="fu">c</span>(prior_know, prior_know_h)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Concerning the last group (h=5), the first 8 variables will be added in s_9 and s_10</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>prior_know <span class="ot">=</span> <span class="fu">c</span>(prior_know, <span class="fu">rep</span>(<span class="fl">0.9</span>,<span class="dv">4</span>),<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">4</span>), <span class="fu">rep</span>(<span class="st">"not used"</span>,<span class="dv">92</span>))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(prior_know) <span class="ot">=</span> <span class="fu">colnames</span>(data[,<span class="sc">-</span><span class="fu">dim</span>(data)[<span class="dv">2</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The vector of prior knowledge is then used to iteratively construct the groups <span class="math inline">G_K</span> and <span class="math inline">G_U</span> for each scenario :</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Groups_construction <span class="ot">&lt;-</span> <span class="cf">function</span> (prior,partial_know){</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prior : vector of the prior knowledge indicating when the variable enters into the G_K group</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># partial_know : proportion of partial knowledge to include (between 0 and 1)</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  G_K <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  G_U <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># G_K = group with a priori</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># G_U = group with no a priori</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(prior)){</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>((prior[j]<span class="sc">&lt;=</span> partial_know)){ </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>      G_K <span class="ot">=</span> <span class="fu">c</span>(G_K, <span class="fu">names</span>(prior)[j])</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>{</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        G_U <span class="ot">=</span> <span class="fu">c</span>(G_U, <span class="fu">names</span>(prior)[j])</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(G_K, G_U)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="estimation-of-the-parameters" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="estimation-of-the-parameters"><span class="header-section-number">4.3</span> Estimation of the parameters</h2>
<p>To select an appropriate value for the parameter <span class="math inline">\lambda</span>, we use, both in semi-LASSO and 0-1 weighted LASSO methods, the <code>cv.glmnet</code> function which performs 10-fold cross-validation. <span class="math inline">K=50</span> datasets are generated following the scheme of Section <a href="#data-simu">4.1</a>. A parameters estimation is performed for each simulation in order to evaluate the variability coming both from the noise <span class="math inline">\epsilon</span> in <a href="#eq-regression" class="quarto-xref">Equation&nbsp;1</a> at the step of data generation and the cross-validation in <code>cv.glmnet</code> . We use both semi-LASSO and 0-1 weighted LASSO to estimate <span class="math inline">\boldsymbol \beta</span> given a dataset and a scenario of <a href="#tbl-scenario" class="quarto-xref">Table&nbsp;1</a>. All the results presented in Section <a href="#results">5</a> are shown as boxplots relative to these <span class="math inline">50</span> replica. The code below shows how the parameters estimation is performed for a particular dataset.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(attempt)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded glmnet 4.1-8</code></pre>
</div>
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>semi_LASSO <span class="ot">&lt;-</span> <span class="cf">function</span>(known_group, unknown_group, data_transform, response_var, <span class="at">inter =</span> <span class="cn">TRUE</span>){</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># known and unknown groupe give the indices of variables for which we have (or not) an a priori</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># data_transform is the dataset containing all covariates X and the response</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># variable Y</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># response_var gives the name of the variable in the dataset to be considered</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># as the response variable</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># inter is TRUE or FALSE to include or not an intercept into the model</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">warn_if</span>(response_var <span class="sc">%in%</span> known_group <span class="sc">||</span>response_var <span class="sc">%in%</span> unknown_group  ,isTRUE,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">msg =</span> <span class="st">"Your response variable will be used as an explicative variable. </span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="st">          Maybe you put it in one of the two groups"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Warn if the response variable is put in G_K or G_U</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">dim</span>(data_transform)[<span class="dv">1</span>]</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  p<span class="ot">=</span> <span class="fu">dim</span>(data_transform)[<span class="dv">2</span>]<span class="sc">-</span><span class="dv">1</span> <span class="co"># remove the response variable</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  G_K <span class="ot">=</span> known_group</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>  G_U <span class="ot">=</span> unknown_group</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>  ind <span class="ot">=</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data_transform)<span class="sc">==</span>response_var)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(G_K)<span class="sc">==</span><span class="dv">0</span>){</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Case when there is no a priori.</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Then it's just a classic LASSO</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">=</span> <span class="fu">cv.glmnet</span>(<span class="fu">as.matrix</span>(data_transform[,<span class="sc">-</span>ind]),<span class="fu">as.matrix</span>(data_transform[,ind]),<span class="at">alpha =</span> <span class="dv">1</span>,<span class="at">grouped=</span><span class="cn">FALSE</span>, <span class="at">intercept =</span> inter)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    lambda <span class="ot">=</span> model<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    coeff <span class="ot">=</span> <span class="fu">coef</span>(model)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    beta_NK <span class="ot">=</span> coeff[<span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(coeff)]</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(beta) <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">colnames</span>(data_transform[,<span class="sc">-</span>ind]),<span class="st">"Intercept"</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(G_U)){</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>      beta[G_U[j]]<span class="ot">=</span> beta_NK[j]</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we add the value of intercept</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    beta[p<span class="sc">+</span><span class="dv">1</span>] <span class="ot">=</span> coeff[<span class="dv">1</span>]</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(G_U)<span class="sc">==</span><span class="dv">0</span>){</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Case when all regressors are known</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Then it's a classic regression (we suppose that the number of regressors </span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># will not exceed the number of observations)</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    list_names_data <span class="ot">=</span> <span class="fu">colnames</span>(data_transform)[G_K]</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    lambda <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">length</span>(list_names_data)<span class="sc">!=</span><span class="dv">0</span>){</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>      reg_j <span class="ot">=</span> <span class="fu">lm</span>(<span class="at">formula =</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(response_var, <span class="st">"~ 0+"</span>, <span class="fu">paste</span>(list_names_data, <span class="at">collapse =</span> <span class="st">"+"</span>))), <span class="at">data =</span> data_transform)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>      resume_j <span class="ot">=</span> reg_j<span class="sc">$</span>coefficients</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> {</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>      <span class="co"># The case when j is an initial node with no parent</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>      resume_j <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,p)</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creation of the final vector of coefficients</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(beta) <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">colnames</span>(data_transform[,<span class="sc">-</span>ind]),<span class="st">"Intercept"</span>)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(G_K)){</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>      beta[G_U[j]]<span class="ot">=</span> resume_j[j]</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> ((<span class="fu">length</span>(G_U)<span class="sc">!=</span><span class="dv">0</span>)<span class="sc">&amp;</span>(<span class="fu">length</span>(G_K)<span class="sc">!=</span><span class="dv">0</span>)){</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Semi-LASSO</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creation of all necessary objects X_NK and X_K</span></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>    X_NK <span class="ot">=</span><span class="fu">as.matrix</span>(data_transform[,G_U]) </span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>    X_K <span class="ot">=</span> <span class="fu">cbind</span>(<span class="fu">as.matrix</span>(data_transform[,G_K]),<span class="fu">rep</span>(<span class="dv">1</span>,n)) <span class="co"># we add a column for the intercept</span></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>    Y <span class="ot">=</span> <span class="fu">as.matrix</span>(data_transform[,ind])</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creation of U and V</span></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>    I_n <span class="ot">=</span> <span class="fu">diag</span>(n)</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>    U <span class="ot">=</span> (I_n <span class="sc">-</span> X_K<span class="sc">%*%</span><span class="fu">solve</span>(<span class="fu">t</span>(X_K)<span class="sc">%*%</span>X_K)<span class="sc">%*%</span><span class="fu">t</span>(X_K))<span class="sc">%*%</span>Y  <span class="co"># new Y</span></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>    V <span class="ot">=</span> X_NK <span class="sc">-</span> X_K<span class="sc">%*%</span><span class="fu">solve</span>(<span class="fu">t</span>(X_K)<span class="sc">%*%</span>X_K)<span class="sc">%*%</span><span class="fu">t</span>(X_K)<span class="sc">%*%</span>X_NK</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Performing first step LASSO, to find beta_NK</span></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>    model_NK <span class="ot">=</span> <span class="fu">cv.glmnet</span>(V,U,<span class="at">alpha =</span> <span class="dv">1</span>,<span class="at">grouped=</span><span class="cn">FALSE</span>, <span class="at">intercept =</span> inter)</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>    lambda <span class="ot">=</span> model_NK<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Getting the estimated coefficients and constructing beta_NK</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Be careful, the first parameter is the intercept in the coef() function</span></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>    coeff_NK <span class="ot">=</span> <span class="fu">coef</span>(model_NK)</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>    beta_NK <span class="ot">=</span> coeff_NK[<span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(coeff_NK)]</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We can deduce beta_K from beta_NK</span></span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>    beta_K <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X_K)<span class="sc">%*%</span>X_K)<span class="sc">%*%</span><span class="fu">t</span>(X_K)<span class="sc">%*%</span>(Y<span class="sc">-</span>X_NK<span class="sc">%*%</span>beta_NK)</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>    beta_K</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Construction of the final beta</span></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(beta) <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">colnames</span>(data_transform[,<span class="sc">-</span>ind]),<span class="st">"Intercept"</span>)</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(G_K)){</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>      beta[G_K[j]]<span class="ot">=</span> beta_K[j]</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(G_U)){</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>      beta[G_U[j]]<span class="ot">=</span> beta_NK[j]</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>    <span class="co">#beta[p+1] = coeff_NK[1]</span></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>    beta[p<span class="sc">+</span><span class="dv">1</span>] <span class="ot">=</span> beta_K[<span class="fu">length</span>(beta_K)]</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(beta,lambda)</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"data/Data.RData"</span>) </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>( <span class="st">"digits"</span><span class="ot">=</span><span class="dv">5</span>, <span class="st">"scipen"</span><span class="ot">=</span><span class="dv">0</span>) </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># loading of the list containing the K = 50 datasets</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># We show how the estimation works using only the first dataset</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>data_K <span class="ot">=</span> Data[[<span class="dv">1</span>]]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># semi-LASSO</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>regressors_list_by_knowledge <span class="ot">=</span> <span class="fu">list</span>() <span class="co"># creation of the list of regressors</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co"># each element of the list will correspond to a particular scenario</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>index <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (l <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)){ <span class="co"># levels of prior knowledge</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  A_priori_group <span class="ot">=</span> <span class="fu">Groups_construction</span>(prior_know,l) <span class="co"># construction of G_K and G_U</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  result <span class="ot">=</span> <span class="fu">semi_LASSO</span>(A_priori_group[[<span class="dv">1</span>]], A_priori_group[[<span class="dv">2</span>]], data_K,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">colnames</span>(data_K)[<span class="fu">length</span>(<span class="fu">colnames</span>(data_K))],</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>                      <span class="at">inter =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  regressors_list_by_knowledge[[index]] <span class="ot">=</span> result[[<span class="dv">1</span>]]</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  index <span class="ot">=</span> index <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(regressors_list_by_knowledge) <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 0-1 weighted LASSO</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>regressors_list_by_knowledge_glmnet <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>index <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (l <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)){</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">=</span> <span class="fu">as.numeric</span>(prior_know<span class="sc">&gt;</span>l) <span class="co"># transformation of prior_know into 0-1 weights</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the p+1th variable in data_K is Y</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>  reg <span class="ot">=</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(data_K[,<span class="sc">-</span>(p<span class="sc">+</span><span class="dv">1</span>)]), <span class="at">y =</span> <span class="fu">as.matrix</span>(data_K[,(p<span class="sc">+</span><span class="dv">1</span>)]), <span class="at">penalty.factor =</span> weights, <span class="at">grouped=</span><span class="cn">FALSE</span>, <span class="at">alpha=</span><span class="dv">1</span>, <span class="at">intercept=</span><span class="cn">TRUE</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>  result <span class="ot">=</span> <span class="fu">coef</span>(reg)[<span class="sc">-</span><span class="dv">1</span>,] <span class="co"># reorder the position of the intercept estimation</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>  result <span class="ot">=</span> <span class="fu">c</span>(result, <span class="fu">coef</span>(reg)[<span class="dv">1</span>,])</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">names</span>(result)[p<span class="sc">+</span><span class="dv">1</span>] <span class="ot">=</span> <span class="st">"Intercept"</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>  regressors_list_by_knowledge_glmnet[[index]] <span class="ot">=</span> result</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>  index <span class="ot">=</span> index <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(regressors_list_by_knowledge_glmnet)<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="co"># The first 8 estimated coefficients by semi-LASSO, for a prior knowledge of 20%</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>regressors_list_by_knowledge[[<span class="dv">3</span>]][<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     V1      V2      V3      V4      V5      V6      V7      V8 
0.41737 2.45429 0.00000 0.00000 0.00000 6.75924 2.24398 3.21918 </code></pre>
</div>
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The first 8 estimated coefficients by 0-1 weighted LASSO, for a prior knowledge of 20%</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>regressors_list_by_knowledge_glmnet[[<span class="dv">3</span>]][<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     V1      V2      V3      V4      V5      V6      V7      V8 
0.18738 2.94932 0.00000 0.00000 0.00000 7.25378 0.39463 2.50380 </code></pre>
</div>
</div>
</section>
<section id="criteria" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="criteria"><span class="header-section-number">4.4</span> Criteria used to compare the methods</h2>
<p>To compare the results from semi-LASSO and the 0-1 weighted LASSO, we use the following indicators:</p>
<ul>
<li><p>The sensitivity (true positive rate), defined as: <span class="math display">\text{se} = \frac{TP}{TP + FN}= \frac{|\{\beta_j\neq0  \} \bigcap \{\widehat{\beta_j}\neq0 \}|}{|\{\beta_j\neq0  \}|},</span> with <span class="math inline">\widehat{\beta_j}</span> the estimated parameters of the model. TP is the number of true positives, i.e.&nbsp;the number of true regressors of the model that are correctly selected by the method, and FN is the number of false negatives, i.e the number of true regressors of the model that are not selected by the method. It measures the proportion of true regressors of the model correctly selected by the method among all the true regressors of the model .</p></li>
<li><p>The specificity (true negative rate), defined as: <span class="math display">\text{sp} = \frac{TN}{TN+FP}\frac{|\{\beta_j=0  \} \bigcap \{\widehat{\beta_j}=0 \}|}{|\{\beta_j=0  \}|},</span> with TN the number of true negatives, i.e.&nbsp;the variables which are not regressors in the model and not found as such by the method, and FP the false positives, i.e.&nbsp;the variables which are not regressors in the model but found as such by the method. It measures the proportion of variables which are not regressors in the model and that the method correctly identify as such.</p></li>
<li><p>The Root Mean Square Error, defined as: <span class="math display">\text{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2},</span> with <span class="math inline">\hat{y}_i = \sum_{j=1}^{p}\widehat{\beta_i}x_i</span> the prediction of <span class="math inline">y_i</span>. It measures the capacity of prediction of the fitted model. It is calculated on a new dataset of size <span class="math inline">n</span>, constructed with the procedure described in Section <a href="#data-simu">4.1</a>. This dataset was not used to perform the parameters estimation.</p></li>
<li><p>Individual coefficients estimation, defined as the gap between the estimated value <span class="math inline">\widehat{\beta_j}</span> and the true one <span class="math inline">\beta_j</span>, to identify which method performs better in finding the true value of the coefficients, and the impact of the prior knowledge on the estimation.</p></li>
</ul>
<p>The first two criteria concern the ability of the fitted model to select the true regressors whereas the third one assesses the capacity to predict <span class="math inline">Y</span>. As for the fourth one, it measures the quality of the estimation of a particular <span class="math inline">\beta_j</span>. We also want to see the impact of the prior knowledge level on these indicators.</p>
</section>
</section>
<section id="results" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Results</h1>
<p>We present here the results of the simulations introduced in Section <a href="#simulations">4</a>.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"data/Data_final.RData"</span>) <span class="co"># load the RData containing</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># - parameters estimations for semi-LASSO and 0-1 weighted LASSO</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># - sensitivity</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># - specificity</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># - RMSE on the test dataset</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="speci-sensi" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="speci-sensi"><span class="header-section-number">5.1</span> Sensitivity and specificity</h2>
<p>We first give the results about the performance of variables selection, which is emphasized by the sensitivity and specificity described in Section <a href="#criteria">4.4</a>.</p>
<p><a href="#fig-sensi" class="quarto-xref">Figure&nbsp;2</a> and <a href="#fig-speci" class="quarto-xref">Figure&nbsp;3</a> present respectively the sensibility and the specifity for the semi-LASSO and 0-1 weighted LASSO methods. Each boxplot corresponds to the <span class="math inline">50</span> replica described in Section <a href="#data-simu">4.1</a>, done for both methods (blue for 0-1 weighted LASSO, yellow for semi-LASSO) and a prior knowledge given in abscissa (see <a href="#tbl-scenario" class="quarto-xref">Table&nbsp;1</a>).<br>
We can first notice that in <a href="#fig-sensi" class="quarto-xref">Figure&nbsp;2</a> the sensitivity increases with prior knowledge for both methods, which seems coherent with the fact that we force relevant variables to be included into the model. With a level of prior knowledge lower than <span class="math inline">80\%</span>, the semi-LASSO method seems to be more efficient in finding true positive regressors. In the semi-LASSO method, we first remove the influence of the variables of <span class="math inline">G_K</span> before solving a LASSO problem on the remaining variables (see the expressions of <span class="math inline">\mathbf u</span> and <span class="math inline">\mathbf v</span> in Section <a href="#semilasso">3</a>). This suggests that this additional step allows the final LASSO procedure to be more efficient in finding true regressors among the remaining variables. When the prior knowledge level is high enough (<span class="math inline">&gt;90\%</span>), the given information is sufficient for both methods to perform equally well in terms of sensitivity.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Data_final, <span class="fu">aes</span>(<span class="at">x =</span> PriorKnowledge, <span class="at">y =</span> Sensitivity)) <span class="sc">+</span> </span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">fill =</span> Method), <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="fl">0.9</span>)) <span class="sc">+</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#00AFBB"</span>, <span class="st">"#E7B800"</span>)) <span class="sc">+</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">1</span>), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span> <span class="fu">expand_limits</span>(<span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"% of prior knowledge"</span>, <span class="at">y =</span> <span class="st">"Sensitivity"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-sensi" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="semi-LASSO_files/figure-html/fig-sensi-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Boxplots of the sensitivity for each prior knowledge scenario and each method of parameters estimation.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Looking at the specificity in <a href="#fig-speci" class="quarto-xref">Figure&nbsp;3</a>, we see that it is high for both methods and prior knowledge levels. The 0-1 weighted LASSO method seems to perform better when the prior knowledge level increases, and to limit the number of false positive into the model. The step of LASSO in the semi-LASSO algorithm is performed in a smaller space, and tries to find non-zero values for some coefficients. When the prior knowledge information increases, there are less and less correct regressors to find, which means the LASSO will include more and more irrelevant variables in the model. It is particularly true when the prior knowledge is set to <span class="math inline">1</span> and all correct regressors have been put in <span class="math inline">G_K</span>. In the 0-1 weighted LASSO, the optimization is done directly in <span class="math inline">\mathbb{R}^{p+1}</span>, which restricts the number of false positives. In both cases, the global specificity never goes below <span class="math inline">0.85</span>, which is satisfying.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Data_final, <span class="fu">aes</span>(<span class="at">x =</span> PriorKnowledge, <span class="at">y =</span> Specificity)) <span class="sc">+</span> </span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">fill =</span> Method), <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="fl">0.9</span>)) <span class="sc">+</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#00AFBB"</span>, <span class="st">"#E7B800"</span>)) <span class="sc">+</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">1</span>),<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span>  <span class="fu">expand_limits</span>(<span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"% of prior knowledge"</span>, <span class="at">y =</span> <span class="st">"Specificity"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-speci" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-speci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="semi-LASSO_files/figure-html/fig-speci-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-speci-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Boxplots of the specificity for each prior knowledge scenario and each method of parameters estimation.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="rmse" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="rmse"><span class="header-section-number">5.2</span> RMSE</h2>
<p>Regarding the RMSE values plotted in <a href="#fig-rmse" class="quarto-xref">Figure&nbsp;4</a>, we can see that the semi-LASSO has a lower prediction error than the 0-1 weighted LASSO for a prior knowledge level smaller than <span class="math inline">80\%</span>. For higher values, the 2 methods are similar, with a slight advantage for the 0-1 weighted LASSO. This can be explained one more time by the irrelevant variables added into the model by the semi-LASSO method when the prior knowledge level is high.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Data_final, <span class="fu">aes</span>(<span class="at">x =</span> PriorKnowledge, <span class="at">y =</span> RMSEtest)) <span class="sc">+</span> </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">fill =</span> Method), <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="fl">0.9</span>)) <span class="sc">+</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#00AFBB"</span>, <span class="st">"#E7B800"</span>)) <span class="sc">+</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"% of prior knowledge"</span>, <span class="at">y =</span> <span class="st">"RMSE"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rmse" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rmse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="semi-LASSO_files/figure-html/fig-rmse-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rmse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Boxplots of the RMSE calculated on a test dataset for each prior knowledge scenario and each method of parameters estimation.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="parameters-estimation" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="parameters-estimation"><span class="header-section-number">5.3</span> Parameters estimation</h2>
<p>To illustrate how the methods perform in estimating the parameters, we plot on <a href="#fig-beta-esti" class="quarto-xref">Figure&nbsp;5</a>, the estimation of the parameter <span class="math inline">\beta_7=7</span>, associated to the variable <span class="math inline">X_7</span>. The horizontal red line indicates the true value of the coefficient, whereas the vertical one shows the first scenario where the variable <span class="math inline">X_7</span> is added to <span class="math inline">G_K</span> (see <a href="#tbl-scenario" class="quarto-xref">Table&nbsp;1</a>).<br>
Until the prior knowledge level reaches <span class="math inline">70\%</span>, the semi-LASSO estimates <span class="math inline">\beta_7</span> using a classic LASSO, as <span class="math inline">X_7</span> is in <span class="math inline">G_U</span>. Nevertheless, other variables are progressively added to <span class="math inline">G_K</span> when the prior knowledge level increases. We can see that the estimation based on the semi-LASSO is better than the one made with the 0-1 weighted LASSO. This difference certainly follows from the fact that the optimization performed on a smaller space in the semi-LASSO than the one from the 0-1 weighted LASSO, performed on <span class="math inline">\mathbb{R}^{p+1}</span>. After we reach <span class="math inline">70\%</span> of prior knowledge, <span class="math inline">X_7</span> belongs to <span class="math inline">G_K</span>, which means it has a <span class="math inline">0</span> weight in the 0-1 weighted LASSO, and is estimated via ordinary least squares estimator in semi-LASSO. At this point, both methods perform equally well for the estimation of <span class="math inline">\beta_7</span>, since we force the variable to be part of the model.<br>
One can notice that for some levels of prior knowledge (<span class="math inline">20\%</span> to <span class="math inline">40\%</span>), <code>glmnet</code> often does not include the variable into the model, i.e.&nbsp;the estimated parameter <span class="math inline">\widehat{\beta_7}</span> is equal to zero, and thus generates some false negatives which impacts the sensitivity on <a href="#fig-sensi" class="quarto-xref">Figure&nbsp;2</a>. This is not the case for the semi-LASSO method.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Data_final, <span class="fu">aes</span>(<span class="at">x =</span> PriorKnowledge, <span class="at">y =</span> V7)) <span class="sc">+</span> </span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">fill =</span> Method), <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="fl">0.9</span>)) <span class="sc">+</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#00AFBB"</span>, <span class="st">"#E7B800"</span>)) <span class="sc">+</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">7</span>), <span class="at">color =</span> <span class="st">"red"</span>)    <span class="sc">+</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="dv">8</span>), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"% of prior knowledge"</span>, <span class="at">y =</span> <span class="st">"X7"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-beta-esti" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-beta-esti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="semi-LASSO_files/figure-html/fig-beta-esti-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-beta-esti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Boxplots of the estimated values for a coefficient of the model, for each prior knowledge scenario and each method of parameters estimation.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="other-tests-and-some-possible-extensions" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="other-tests-and-some-possible-extensions"><span class="header-section-number">5.4</span> Other tests and some possible extensions</h2>
<p>Similar results as the ones presented in Section <a href="#results">5</a> were obtained with different settings including :</p>
<ul>
<li>The number of observations <span class="math inline">n</span> being set to <span class="math inline">1000</span> to have <span class="math inline">p&lt;n</span>.</li>
<li>Independent variables <span class="math inline">(X_j, \, j \in [1,p])</span> instead of correlated ones.</li>
<li>A model without intercept<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</li>
<li>Non-centered variables <span class="math inline">(X_j, \, j \in [1,p])</span>.</li>
<li>Different values for <span class="math inline">\sigma</span>.</li>
</ul>
<p>In all cases, the obtained graphs show similar behaviors as the ones presented here and are thus not included in the paper.<br>
Several extensions are possible using the framework of the semi-LASSO method. We can think at first of a weighted elasticnet method <span class="citation" data-cites="zou2005regularization">(<a href="#ref-zou2005regularization" role="doc-biblioref">Zou and Hastie 2005</a>)</span>, with <span class="math inline">0</span> and <span class="math inline">1</span> weights. The mathematical formulation presented in Section <a href="#semilasso">3</a> can indeed be generalized to this type of penalization.<br>
Another possible extension would be a classic weighted LASSO with some <span class="math inline">0</span> weights and other weights not all equal to <span class="math inline">1</span>. In this case, we could again use the decomposition of Section <a href="#semilasso">3</a>, but the classical LASSO in the semi-LASSO algorithm would be replaced by a weighted LASSO.</p>
</section>
</section>
<section id="conclusion" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion</h1>
<p>This paper introduces a new weighted LASSO method, called semi-LASSO, designed for the integration of prior knowledge into the model. It relies on the <code>R</code> package <code>glmnet</code>, but does not use the <code>penalty.factor</code> allowing to specify weights. Instead of that, it first transforms the problem to divide the optimization procedure into two steps. The first one is an ordinary least square method, which allows to reduce the space dimension for the second one: a classic LASSO procedure. The reduction of dimension in the semi-LASSO procedure gives significantly better results than the use of <code>penalty.factor</code> implemented in <code>glmnet</code> both on the proportion of true regressors detected by the method and on the prediction error. We can also see a better coefficient estimation with the semi-LASSO. These improvements come with a size restriction: our method can only be applied if <span class="math inline">|G_K|\leq n</span> because the first step is an ordinary least squares problem.<br>
Moreover, in terms of model selection, increasing the true positive rate using the semi-LASSO means decreasing the true negative rate, compared to the 0-1 weighted LASSO. Depending on the problem the user tries to solve, it is left to his discretion to use either semi-LASSO or <code>glmnet</code> to reduce the most important rate at his eyes. The semi-LASSO procedure can also be applied in some extensions like weighted LASSO with different weights, or an elastic-net penalization.</p>
</section>
<section id="acknowlegments" class="level1 unnumbered">
<h1 class="unnumbered">Acknowlegments</h1>
<p>The authors thank the Région Grand Est, Project LOR-IA THESE for funding the PhD thesis of Anouk Rago.</p>
</section>
<section id="session-information" class="level1 unnumbered">
<h1 class="unnumbered">Session information</h1>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>R version 4.2.2 (2022-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 22.04.4 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so

locale:
 [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       
 [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   
 [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          
[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
[1] ggplot2_3.4.4  glmnet_4.1-8   attempt_0.3.1  expm_0.999-8   Matrix_1.6-1.1
[6] corrplot_0.92  mvtnorm_1.2-4 

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.11      compiler_4.2.2   pillar_1.9.0     iterators_1.0.14
 [5] tools_4.2.2      digest_0.6.33    jsonlite_1.8.8   evaluate_0.23   
 [9] lifecycle_1.0.4  tibble_3.2.1     gtable_0.3.4     lattice_0.21-9  
[13] pkgconfig_2.0.3  rlang_1.1.2      foreach_1.5.2    cli_3.6.2       
[17] yaml_2.3.8       xfun_0.41        fastmap_1.1.1    withr_2.5.2     
[21] knitr_1.45       vctrs_0.6.5      grid_4.2.2       glue_1.6.2      
[25] R6_2.5.1         fansi_1.0.6      survival_3.5-7   rmarkdown_2.25  
[29] farver_2.1.1     magrittr_2.0.3   scales_1.3.0     codetools_0.2-19
[33] htmltools_0.5.7  splines_4.2.2    shape_1.4.6      colorspace_2.1-0
[37] renv_1.0.3       labeling_0.4.3   utf8_1.2.4       munsell_0.5.0   </code></pre>
</div>
</div>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">Bibliography</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bergersen2011weighted" class="csl-entry" role="listitem">
Bergersen, Linn Cecilie, Ingrid K Glad, and Heidi Lyng. 2011. <span>“Weighted Lasso with Data Integration.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 10 (1).
</div>
<div id="ref-cassan2023optimizing" class="csl-entry" role="listitem">
Cassan, Océane, Charles Henri Lecellier, Antoine Martin, Laurent Brehelin, and Sophie Lèbre. 2023. <span>“Optimizing Data Integration Improves Gene Regulatory Network Inference in Arabidopsis Thaliana.”</span> <em>bioRxiv</em>, 2023–09.
</div>
<div id="ref-charbonnier2010weighted" class="csl-entry" role="listitem">
Charbonnier, Camille, Julien Chiquet, and Christophe Ambroise. 2010. <span>“Weighted-LASSO for Structured Network Inference from Time Course Data.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 9 (1).
</div>
<div id="ref-efron2004least" class="csl-entry" role="listitem">
Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. <span>“<span class="nocase">Least angle regression</span>.”</span> <em>The Annals of Statistics</em> 32 (2): 407–99.
</div>
<div id="ref-friedman2010regularization" class="csl-entry" role="listitem">
Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. 2010. <span>“Regularization Paths for Generalized Linear Models via Coordinate Descent.”</span> <em>Journal of Statistical Software</em> 33 (1): 1.
</div>
<div id="ref-friguet2010impact" class="csl-entry" role="listitem">
Friguet, Chloé. 2010. <span>“Impact de La d<span>é</span>pendance Dans Les Proc<span>é</span>dures de Tests Multiples En Grande Dimension.”</span> PhD thesis, Agrocampus-Ecole nationale sup<span>é</span>rieure d’agronomie de rennes.
</div>
<div id="ref-greenfield2013robust" class="csl-entry" role="listitem">
Greenfield, Alex, Christoph Hafemeister, and Richard Bonneau. 2013. <span>“Robust Data-Driven Incorporation of Prior Knowledge into the Inference of Dynamic Regulatory Networks.”</span> <em>Bioinformatics</em> 29 (8): 1060–67.
</div>
<div id="ref-grzegorczyk2019overview" class="csl-entry" role="listitem">
Grzegorczyk, Marco, Andrej Aderhold, and Dirk Husmeier. 2019. <span>“Overview and Evaluation of Recent Methods for Statistical Inference of Gene Regulatory Networks from Time Series Data.”</span> <em>Gene Regulatory Networks: Methods and Protocols</em>, 49–94.
</div>
<div id="ref-huynh2019gene" class="csl-entry" role="listitem">
Huynh-Thu, Vân Anh, and Guido Sanguinetti. 2019. <span>“Gene Regulatory Network Inference: An Introductory Survey.”</span> <em>Gene Regulatory Networks: Methods and Protocols</em>, 1–23.
</div>
<div id="ref-leclerc2008survival" class="csl-entry" role="listitem">
Leclerc, Robert D. 2008. <span>“Survival of the Sparsest: Robust Gene Networks Are Parsimonious.”</span> <em>Molecular Systems Biology</em> 4 (1): 213.
</div>
<div id="ref-siahpirani2019integrative" class="csl-entry" role="listitem">
Siahpirani, Alireza Fotuhi, Deborah Chasman, and Sushmita Roy. 2019. <span>“Integrative Approaches for Inference of Genome-Scale Gene Regulatory Networks.”</span> <em>Gene Regulatory Networks: Methods and Protocols</em>, 161–94.
</div>
<div id="ref-tibshirani1996regression" class="csl-entry" role="listitem">
Tibshirani, Robert. 1996. <span>“Regression Shrinkage and Selection via the Lasso.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 58 (1): 267–88.
</div>
<div id="ref-xenarios2000dip" class="csl-entry" role="listitem">
Xenarios, Ioannis, Danny W Rice, Lukasz Salwinski, Marisa K Baron, Edward M Marcotte, and David Eisenberg. 2000. <span>“DIP: The Database of Interacting Proteins.”</span> <em>Nucleic Acids Research</em> 28 (1): 289–91.
</div>
<div id="ref-yuan2006model" class="csl-entry" role="listitem">
Yuan, Ming, and Yi Lin. 2006. <span>“Model Selection and Estimation in Regression with Grouped Variables.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 68 (1): 49–67.
</div>
<div id="ref-zou2006adaptive" class="csl-entry" role="listitem">
Zou, Hui. 2006. <span>“The Adaptive Lasso and Its Oracle Properties.”</span> <em>Journal of the American Statistical Association</em>, 1418–29.
</div>
<div id="ref-zou2005regularization" class="csl-entry" role="listitem">
Zou, Hui, and Trevor Hastie. 2005. <span>“Regularization and Variable Selection via the Elastic Net.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 67 (2): 301–20.
</div>
</div>
<!-- -->

</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p> During the redaction of this paper, we tested several options in <code>glmnet</code>, in particular the <code>intercept</code> parameter that we set to FALSE. It seems that when the data given in entry of the function is not centered beforehands, <code>glmnet</code> does not produce convincing results. We then found ourselves with significantly better results using the semi-LASSO algorithm. It appears that this <a href="https://stackoverflow.com/questions/49495494/glmnet-is-different-with-intercept-true-compared-to-intercept-false-and-with-pen">issue</a> was raised years ago, but do not seem to be fixed. <a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{champagnat2023,
  author = {Champagnat, Nicolas and Gégout-Petit, Anne and Rago, Anouk},
  title = {Semi-Lasso: A Weighted {Lasso} Designed for the Integration
    of Known Regressors in Linear Model},
  journal = {Computo},
  date = {2023-01-02},
  url = {https://github.com/computorg/computo-quarto-extension},
  doi = {10.xxxx/xxx-xxx},
  issn = {2824-7795},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-champagnat2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Champagnat, Nicolas, Anne Gégout-Petit, and Anouk Rago. 2023.
<span>“Semi-Lasso: A Weighted Lasso Designed for the Integration of
Known Regressors in Linear Model.”</span> <em>Computo</em>, January. <a href="https://doi.org/10.xxxx/xxx-xxx">https://doi.org/10.xxxx/xxx-xxx</a>.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb25" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Semi-Lasso: a weighted Lasso designed for the integration of known regressors in linear model"</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">#subtitle: "To be used as template for contribution to Computo"</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 01/02/2023</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Nicolas Champagnat</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">    corresponding: false</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">    email: nicolas.champagnat@inria.fr</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université de Lorraine, CNRS, Inria, IECL, UMR 7502</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co">        city: Nancy</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co">        country: France</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Anne Gégout-Petit</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co">    corresponding: false</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co">    email: anne.gegout-petit@univ-lorraine.fr</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université de Lorraine, CNRS, Inria, IECL, UMR 7502</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="co">        city: Nancy</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="co">        country: France</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Anouk Rago</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="co">    corresponding: true</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="co">    email: anouk.rago@univ-lorraine.fr</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université de Lorraine, CNRS, Inria, IECL, UMR 7502</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="co">        city: Nancy</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="co">        country: France</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> |</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a><span class="co">  This document provides a template based on the quarto system for contributions to Computo. The github repository in itself provides a specific quarto extension useful for authors (and editors!).</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span><span class="co"> [regression, LASSO, prior knowledge, R, gene network inference]</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a><span class="an">doi:</span><span class="co"> 10.xxxx/xxx-xxx</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="co">  type: article-journal</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a><span class="co">  container-title: "Computo"</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a><span class="co">  doi: "10.xxxx/xxx-xxx"</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a><span class="co">  url: "https://github.com/computorg/computo-quarto-extension"</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a><span class="co">  issn: "2824-7795"</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a><span class="an">google-scholar:</span><span class="co"> true</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> computorg</span></span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> "computo-quarto-extension"</span></span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> true # set to false once the build is running</span></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> false # will be set to true once accepted</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html: default</span></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a><span class="fu"># Abstract</span></span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>To encode prior information in a regression problem, statisticians may use a weighted LASSO, in which variables can have different weights in the penalization. In this paper, we propose an alternative method called semi-LASSO, which solves a specific case of weighted LASSO designed for the integration of known regressors in linear model. The optimization procedure is divided in two steps: the first one is an ordinary least squares method and the second one a classic LASSO procedure in lower dimension. Numerical experiments are performed on synthetic data to compare the performances of this new method with the usual weighted LASSO implemented in <span class="in">`glmnet`</span>. The results show an improvement of the sensitivity, the variable selection and the prediction capability when using semi-LASSO. </span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction {#introduction}</span></span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>The LASSO <span class="co">[</span><span class="ot">@tibshirani1996regression</span><span class="co">]</span> is a widely used technique when it comes to perform both estimation of parameters and variable selection for a linear model. The penalty on the $l_1$ norm allows indeed to shrink some coefficients to $0$. Moreover, it allows the user to handle cases where the number of variables $p$ is greater than the number of observations $n$ and thus is particularly convenient when one has to deal with large datasets like genomic data. This is why a lot of methods using LASSO and its derivative, like the adaptive LASSO <span class="co">[</span><span class="ot">@zou2006adaptive</span><span class="co">]</span> or group LASSO <span class="co">[</span><span class="ot">@yuan2006model</span><span class="co">]</span> have been developed in particular to infer gene regulatory networks <span class="co">[</span><span class="ot">@siahpirani2019integrative; @grzegorczyk2019overview</span><span class="co">]</span>, which depict the relationships between genes. In these references, a regression model for each gene $j \in <span class="co">[</span><span class="ot">1,p</span><span class="co">]</span>$ is built, giving for each gene a list of regressors linked to it in the network. But inferring this network using only transcriptomic data can be a delicate task due to the massive amount of genes ($p\approx 20 000$) present in the data compared to the small number of samples $n$: the LASSO is then a good tool to select the most relevant regressors among the genes.  </span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>In some cases, some prior biological information can be available and therefore be integrated in the model to improve the results. For example, protein-protein interactions are experimentaly tested and the results are publicly available</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>on the online databases of interacting proteins <span class="co">[</span><span class="ot">@xenarios2000dip</span><span class="co">]</span>. When using a LASSO, one way to perform prior information integration is to specify different penalty strengths for each gene/variable during the estimation of parameters, which is referred as the weighted LASSO and has been applied in various references. @bergersen2011weighted use two different weights to add prior knowledge in their model : the first one is based on the correlation between gene expression and gene copy numbers, the second one relies on the association of gene copy number with survival. A weighted graphical LASSO is performed by @charbonnier2010weighted where prior information on the topology of the network is used. @greenfield2013robust rely on the elastic-net algorithm <span class="co">[</span><span class="ot">@zou2005regularization</span><span class="co">]</span>, and modifies the $l_1$ norm to add prior knowledge on genes interactions. Moreover, it has been shown that optimizing the amount of prior knowledge included into the model gives significantly better results <span class="co">[</span><span class="ot">@cassan2023optimizing</span><span class="co">]</span>. </span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>From a larger perspective, specifying different penalty weights can be useful in any regression problem where prior information on the potential regressors is available. The <span class="in">`R`</span> package <span class="in">`glmnet`</span> <span class="co">[</span><span class="ot">@friedman2010regularization</span><span class="co">]</span> allows the user to do it, by tuning the <span class="in">`penalty.factor`</span> parameter.   </span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>In this paper, we focus on a penalized linear regression model with prior information. We assume that this prior knowledge takes the form of a certainty that some potential regressors must belong to the model. In this particular case, their penalty strength is set to $0$ as we want to be sure to include them in the model. Concerning the other potential regressors for which we have no information, we assume that their penalty strength is the same for all. The number of known regressors should not exceed the number of observations $n$, otherwise the method we propose would be inapplicable. As opposed to the procedure used in <span class="in">`glmnet`</span>, which consists in optimizing the objective function directly with a cyclical coordinate descent method, we propose to first transform the problem to divide the optimization procedure in two steps, one which is an ordinary least squares method and the second one being a classic LASSO procedure in lower dimension. We will see that this procedure actually improves the parameters estimation, the true positive variables selection and reduces the error of prediction.   </span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>The paper is organized as follows. In Section <span class="co">[</span><span class="ot">2</span><span class="co">](#LASSO)</span>, we introduce the LASSO and weighted LASSO, and explain how to deal with the <span class="in">`glmnet`</span> package. Section <span class="co">[</span><span class="ot">3</span><span class="co">](#semilasso)</span> presents our method called semi-LASSO. Finally, we compare the performance of <span class="in">`glmnet`</span> and semi-LASSO on simulated datasets in Section <span class="co">[</span><span class="ot">4</span><span class="co">](#simulations)</span> and discuss the results in Section <span class="co">[</span><span class="ot">5</span><span class="co">](#results)</span>.</span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a><span class="fu"># LASSO and weighted LASSO {#LASSO}</span></span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>We suppose that we have a set of $p$ explicative variables $(X_1,\dots,X_p) \in \mathbb{R}^p$ and a response variable $Y \in \mathbb{R}$ such that:</span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>Y = \beta_0 + \sum_{j=1}^p \beta_jX_j  + \epsilon \,,</span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>$$ {#eq-regression}</span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>with $\epsilon \sim \mathcal{N}(0,\sigma^2)$ a centered noise and $\boldsymbol \beta = (\beta_0,\beta_1,...,\beta_p) \in \mathbb{R}^{p+1}$ the coefficients of the model, potentially equal to zero. We also assume having $n$ realizations of $X_1,\dots,X_p$ and $Y$: we note $\mathbf x = (x_{ij})$ the $n \times (p+1)$ matrix of observations, the first column being filled with ones, and $\mathbf y \in \mathbb{R}^n$ the observations of $Y$. Then the penalized LASSO regression is: </span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>\min_{(\beta_0,\beta_1,\dots,\beta_p)} \quad \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2 + \lambda \text{P}(\boldsymbol\beta) \,,</span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>$$ {#eq-lassofunction}</span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a>with $\lambda$ a positive regularization parameter and $\text{P}(\boldsymbol\beta)$ the LASSO penalty defined by:</span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>\text{P}(\boldsymbol\beta) =\sum_{j=1}^p|\beta_j| \,.</span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a>$${#eq-penalty}</span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>The bigger $\lambda$ is, the more the coefficients $\boldsymbol \beta$ are penalized, which leads to more $0$ coefficients and a sparser model. To solve @eq-lassofunction, two methods can be used. The first one, called the LARS algorithm <span class="co">[</span><span class="ot">@efron2004least</span><span class="co">]</span> and implemented in the <span class="in">`R`</span> package <span class="in">`lars`</span>, relies on the correlation between the explicative variables and $Y$, and is not discussed here. The second one is a coordinate gradient descent directly applied on the objective function, as done in the <span class="in">`R`</span> package <span class="in">`glmnet`</span> <span class="co">[</span><span class="ot">@friedman2010regularization</span><span class="co">]</span>.</span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a>In @eq-penalty, all $\beta_j$ coefficients are treated the same way and are equally penalized, as the data is beforehand normalized before solving the optimization problem in <span class="in">`glmnet`</span>. A possible extension for the LASSO is to add a different weight for each variable. The objective lying behind these weights can either be:</span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>to reduce the conflict between optimal prediction and consistent variable selection, thus achieving some theoretical properties. This is the adaptive LASSO, introduced by @zou2006adaptive. In this case, the weights are  first determined and linked to the data, then iteratively refined to reach some oracle properties. </span>
<span id="cb25-85"><a href="#cb25-85" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>to add some prior information on the potential regressors of the model <span class="co">[</span><span class="ot">@bergersen2011weighted;@charbonnier2010weighted;@greenfield2013robust</span><span class="co">]</span>, as detailed in Section <span class="co">[</span><span class="ot">1</span><span class="co">](#introduction)</span>. This method is called the weighted LASSO. This variant of the LASSO can also resolve the issue raised by variables with similar profiles: only one of them is generally included into the model, not always the most relevant one. By including prior knowledge and so different weights in the objective function, the optimization step leads to a more relevant variables selection.</span>
<span id="cb25-86"><a href="#cb25-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-87"><a href="#cb25-87" aria-hidden="true" tabindex="-1"></a>In these cases, the penalty function becomes:</span>
<span id="cb25-88"><a href="#cb25-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-89"><a href="#cb25-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-90"><a href="#cb25-90" aria-hidden="true" tabindex="-1"></a>\text{P}_{\boldsymbol w}\,(\boldsymbol\beta) =\sum_{j=1}^p w_j\,|\beta_j| \,.</span>
<span id="cb25-91"><a href="#cb25-91" aria-hidden="true" tabindex="-1"></a>$${#eq-weighted-penalty}</span>
<span id="cb25-92"><a href="#cb25-92" aria-hidden="true" tabindex="-1"></a>where $\boldsymbol w = (w_1,\dots,w_p) \in (\mathbb{R}^+)^p$ is the weights vector. A coefficient with a small weight will be less penalized than a coefficient with a big one, and is more likely to be included into the final model. This new problem can be solved with the same gradient descent method of <span class="in">`glmnet`</span> used to solve @eq-lassofunction. When using this <span class="in">`R`</span> package, it is possible to specify particular weights with the <span class="in">`penalty.factor`</span> parameter. </span>
<span id="cb25-93"><a href="#cb25-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-94"><a href="#cb25-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-95"><a href="#cb25-95" aria-hidden="true" tabindex="-1"></a><span class="fu"># Semi-LASSO, a specific weighted LASSO with a priori knowledge {#semilasso}</span></span>
<span id="cb25-96"><a href="#cb25-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-97"><a href="#cb25-97" aria-hidden="true" tabindex="-1"></a><span class="fu">## Adding prior information into the model</span></span>
<span id="cb25-98"><a href="#cb25-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-99"><a href="#cb25-99" aria-hidden="true" tabindex="-1"></a>Linear regression models with penalization are very used in the field of gene network inference <span class="co">[</span><span class="ot">@siahpirani2019integrative; @grzegorczyk2019overview;@huynh2019gene</span><span class="co">]</span>. First because they can deal with big data especially when $n\ll p$, then because they can select only a few interesting genes as regressors. Biologists are indeed aware that gene regulatory networks are sparse <span class="co">[</span><span class="ot">@leclerc2008survival</span><span class="co">]</span>, which makes of the LASSO an interesting method as it offers the possibility to perform variable selection among thousands of genes. But today, a lot of biological information is available concerning the links existing between genes. This information can be added in the model  by using a weighted LASSO as described in Section <span class="co">[</span><span class="ot">2</span><span class="co">](#LASSO)</span>, with carefully chosen weights.  </span>
<span id="cb25-100"><a href="#cb25-100" aria-hidden="true" tabindex="-1"></a>For this purpose, we focus in this paper on a specific case of weighted LASSO, where the weights can be either $0$, which means no penalization, or $1$, as in the usual LASSO. Concretely, a weight equals to $0$ means that the variable associated to this weight will always be included in the model: the associated regression coefficient $\beta_j$ will generically be non-zero. It corresponds to the variable for which we are certain that it has a relationship with $Y$, for example a known protein-protein interaction.  </span>
<span id="cb25-101"><a href="#cb25-101" aria-hidden="true" tabindex="-1"></a>The variables $\mathbf X$ are then divided into two groups:  </span>
<span id="cb25-102"><a href="#cb25-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-103"><a href="#cb25-103" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$G_K$  -K for Known-, the set of variables for which we are certain they are part of the model, as we have prior knowledge on their relationship with $Y$. The method does not penalize these variables and their weight will be set to $0$.   </span>
<span id="cb25-104"><a href="#cb25-104" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$G_{U}$  -U for Unknown-, the set of variables for which we have no prior information on their relationship with $Y$. Their weights will be set to $1$, each of them being penalized the same way by the parameter $\lambda$.  </span>
<span id="cb25-105"><a href="#cb25-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-106"><a href="#cb25-106" aria-hidden="true" tabindex="-1"></a>As it seems reasonable to assume the presence of an intercept in the majority of real data modeling, the variable corresponding to the parameter $\beta_0$ in @eq-regression is always set in $G_K$. The sum of cardinals of the two groups is $G_K$ and $G_U$ is $p+1$ and we also assume in what follows that the number of variables in $G_K$ never exceeds $n$, otherwise the method we propose would be inapplicable. </span>
<span id="cb25-107"><a href="#cb25-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-108"><a href="#cb25-108" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical formulation</span></span>
<span id="cb25-109"><a href="#cb25-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-110"><a href="#cb25-110" aria-hidden="true" tabindex="-1"></a>Using the notations above and  those of Section <span class="co">[</span><span class="ot">2</span><span class="co">](#LASSO)</span>, the problem we want to find the argmin in the following minimization problem:</span>
<span id="cb25-111"><a href="#cb25-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-112"><a href="#cb25-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-113"><a href="#cb25-113" aria-hidden="true" tabindex="-1"></a>\min_{(\beta_0,\beta_1,\dots,\beta_p)} \quad \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2  + \lambda \text{P}_{\boldsymbol 1}(\boldsymbol\beta)\,,</span>
<span id="cb25-114"><a href="#cb25-114" aria-hidden="true" tabindex="-1"></a>$$ {#eq-semilasso}</span>
<span id="cb25-115"><a href="#cb25-115" aria-hidden="true" tabindex="-1"></a>with :</span>
<span id="cb25-116"><a href="#cb25-116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-117"><a href="#cb25-117" aria-hidden="true" tabindex="-1"></a>\text{P}_{\boldsymbol 1}\,(\boldsymbol\beta)  = \sum_{j=1}^p\,\mathbb{1}_{X_j \in G_U}|\beta_j|= \sum_{j,\,X_j \in G_U}\,|\beta_j| \,. <span class="sc">\\</span></span>
<span id="cb25-118"><a href="#cb25-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-119"><a href="#cb25-119" aria-hidden="true" tabindex="-1"></a>We note $\boldsymbol \beta_K$ (respectively $\boldsymbol \beta_U$), the vector $\boldsymbol \beta_K = (\beta_j, \,X_j \in G_K)$ (respectively $\boldsymbol \beta_U = (\beta_j, \,X_j \in G_U)$). In a similar way, we denote by $\mathbf x_K$ (respectively $\mathbf x_U$) the matrix containing the $n$ observations of the variables in $G_K$ (respectively $G_U$).</span>
<span id="cb25-120"><a href="#cb25-120" aria-hidden="true" tabindex="-1"></a>@eq-semilasso can be rewritten as: </span>
<span id="cb25-121"><a href="#cb25-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-122"><a href="#cb25-122" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-123"><a href="#cb25-123" aria-hidden="true" tabindex="-1"></a>\min_{\boldsymbol \beta_U} \big( \min_{\boldsymbol \beta_K} \,\big( \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2 \big)\,+ \lambda \sum_{j,\,X_j \in G_U}\,|\beta_j| \big) \,.</span>
<span id="cb25-124"><a href="#cb25-124" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb25-125"><a href="#cb25-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-126"><a href="#cb25-126" aria-hidden="true" tabindex="-1"></a>The minimum in $\boldsymbol \beta_K$ takes the form of :</span>
<span id="cb25-127"><a href="#cb25-127" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-128"><a href="#cb25-128" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb25-129"><a href="#cb25-129" aria-hidden="true" tabindex="-1"></a>\min_{\boldsymbol \beta_K} \, \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2  </span>
<span id="cb25-130"><a href="#cb25-130" aria-hidden="true" tabindex="-1"></a>&amp; = \min_{\boldsymbol \beta_K} \, \frac{1}{2n}\sum_{i=1}^n(y_i-\beta_0- \sum_{j,\,X_j \in G_U} x_{ij}\beta_j -\sum_{j,\,X_j \in G_K} x_{ij}\beta_j)^2<span class="sc">\\</span></span>
<span id="cb25-131"><a href="#cb25-131" aria-hidden="true" tabindex="-1"></a>&amp;= \min_{\boldsymbol \beta_K} \, \frac{1}{2n}\sum_{i=1}^n(z_i-\beta_0 -\sum_{j,\,X_j \in G_K} x_{ij}\beta_j)^2 \,,</span>
<span id="cb25-132"><a href="#cb25-132" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb25-133"><a href="#cb25-133" aria-hidden="true" tabindex="-1"></a>$${#eq-mini}</span>
<span id="cb25-134"><a href="#cb25-134" aria-hidden="true" tabindex="-1"></a>with $\mathbf z = (z_i)_{1\leq i \leq n} = (y_i-\sum\limits_{j \in G_{U}}x_{ij}\beta_j)_{1\leq i \leq n} = \mathbf y-\mathbf x_{U}\boldsymbol\beta_{U}$  . The minimum above is reached when $\boldsymbol \beta_K$ is the ordinary least squares estimator, under the assumption that $\mathbf x_K^t\mathbf x_K$ is invertible, which implies in particular that the cardinal of $G_K$ is lower than $n$. We have:</span>
<span id="cb25-135"><a href="#cb25-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-136"><a href="#cb25-136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-137"><a href="#cb25-137" aria-hidden="true" tabindex="-1"></a>\text{}\widehat{\boldsymbol\beta_K}=\widehat{\boldsymbol\beta_K}(\boldsymbol\beta_U) = (\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t\mathbf z \,.</span>
<span id="cb25-138"><a href="#cb25-138" aria-hidden="true" tabindex="-1"></a>$$ {#eq-betaK}</span>
<span id="cb25-139"><a href="#cb25-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-140"><a href="#cb25-140" aria-hidden="true" tabindex="-1"></a>Using this expression for $\widehat{\boldsymbol\beta_K}$, @eq-mini becomes: </span>
<span id="cb25-141"><a href="#cb25-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-142"><a href="#cb25-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-143"><a href="#cb25-143" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb25-144"><a href="#cb25-144" aria-hidden="true" tabindex="-1"></a>\min_{\boldsymbol \beta_K} \, \frac{1}{2n}||\boldsymbol y-\boldsymbol x\boldsymbol \beta||^2_2  &amp;=\frac{1}{2n}|| \mathbf y-\mathbf x_{U}\boldsymbol\beta_U-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t(\mathbf y-\mathbf x_{U}\boldsymbol\beta_U)  ||_2^2 <span class="sc">\\</span></span>
<span id="cb25-145"><a href="#cb25-145" aria-hidden="true" tabindex="-1"></a>    &amp;= \frac{1}{2n}||  (I_n-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t)\mathbf y-(\mathbf x_{U}-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t\mathbf x_{U})\boldsymbol\beta_U||^2_2 <span class="sc">\\</span></span>
<span id="cb25-146"><a href="#cb25-146" aria-hidden="true" tabindex="-1"></a>    &amp;= \frac{1}{2n}|| \mathbf u -  \mathbf v \boldsymbol \beta_U||^2_2 \,,</span>
<span id="cb25-147"><a href="#cb25-147" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb25-148"><a href="#cb25-148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-149"><a href="#cb25-149" aria-hidden="true" tabindex="-1"></a>with $\mathbf u = (I_n-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t)\mathbf y$ and $\mathbf v = \mathbf x_{U}-\mathbf x_K(\mathbf x_K^T\mathbf x_K)^{-1}\mathbf x_K^t\mathbf x_{U}$. @eq-semilasso becomes :</span>
<span id="cb25-150"><a href="#cb25-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-151"><a href="#cb25-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-152"><a href="#cb25-152" aria-hidden="true" tabindex="-1"></a>\min_{\boldsymbol \beta_U} \big( \frac{1}{2n}|| \mathbf u - \mathbf v \boldsymbol \beta_U||^2_2 + \lambda \sum_{j,\,X_j \in G_U}\,|\beta_j| \big) \,.</span>
<span id="cb25-153"><a href="#cb25-153" aria-hidden="true" tabindex="-1"></a>$$ {#eq-rewritten-semilasso}</span>
<span id="cb25-154"><a href="#cb25-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-155"><a href="#cb25-155" aria-hidden="true" tabindex="-1"></a>Hence the minimization problem above corresponds to the classic objective function of the LASSO, and only depends on $\boldsymbol \beta_U$. The solution of @eq-rewritten-semilasso can thus be found numerically, using <span class="in">`glmnet`</span> for example. Once the solution $\widehat{\boldsymbol \beta_U}$ is computed, we can obtain $\widehat{\boldsymbol\beta_K}$ by injecting the obtained value for $\widehat{\boldsymbol \beta_U}$ in @eq-betaK. The pseudocode corresponding to this procedure of estimation, called semi-LASSO, is given below in @alg-semilasso.</span>
<span id="cb25-156"><a href="#cb25-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-157"><a href="#cb25-157" aria-hidden="true" tabindex="-1"></a>An alternative solution would be to use <span class="in">`glmnet`</span> directly, by specifying $0$ and $1$ weights in the <span class="in">`penalty.factor`</span> argument: this procedure will be refered as 0-1 weighted LASSO in what follows.   </span>
<span id="cb25-158"><a href="#cb25-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-159"><a href="#cb25-159" aria-hidden="true" tabindex="-1"></a>The semi-LASSO method can be more efficient than the 0-1 weighted LASSO for the following reasons. By separating the variables in two distinct groups before numerically optimizing the objective function @eq-semilasso, we find ourselves solving two different minimization problems. The first one is an ordinary least squares regression, which has a well-known analytical solution, and the second one is a classic LASSO regression. This second problem will be solved numerically, but in a space of smaller dimension (less variables) compared to the 0-1 weighted LASSO. We will see in the next sections that our semi-LASSO method actually improves the parameter estimation, the sensititvity and reduces the error of prediction.</span>
<span id="cb25-160"><a href="#cb25-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-161"><a href="#cb25-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-162"><a href="#cb25-162" aria-hidden="true" tabindex="-1"></a><span class="in">```pseudocode</span></span>
<span id="cb25-163"><a href="#cb25-163" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: alg-semilasso</span></span>
<span id="cb25-164"><a href="#cb25-164" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-indent-size: "1.2em"</span></span>
<span id="cb25-165"><a href="#cb25-165" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-comment-delimiter: "//"</span></span>
<span id="cb25-166"><a href="#cb25-166" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-line-number: true</span></span>
<span id="cb25-167"><a href="#cb25-167" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-line-number-punc: ":"</span></span>
<span id="cb25-168"><a href="#cb25-168" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-no-end: false</span></span>
<span id="cb25-169"><a href="#cb25-169" aria-hidden="true" tabindex="-1"></a><span class="in">#| pdf-placement: "H"</span></span>
<span id="cb25-170"><a href="#cb25-170" aria-hidden="true" tabindex="-1"></a><span class="in">#| pdf-line-number: true</span></span>
<span id="cb25-171"><a href="#cb25-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-172"><a href="#cb25-172" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb25-173"><a href="#cb25-173" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Semi-LASSO}</span></span>
<span id="cb25-174"><a href="#cb25-174" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb25-175"><a href="#cb25-175" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Inputs}: \\</span></span>
<span id="cb25-176"><a href="#cb25-176" aria-hidden="true" tabindex="-1"></a><span class="in">     $\quad$ $G_K$ and $G_U$ the groups of variables  \\</span></span>
<span id="cb25-177"><a href="#cb25-177" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ $\mathbf x \in \mathcal{M}_{n \times p}(\mathbb{R})$ the data matrix \\</span></span>
<span id="cb25-178"><a href="#cb25-178" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ $\mathbf y \in \mathbb{R}^n$ the vector of observed values for $Y$ \\</span></span>
<span id="cb25-179"><a href="#cb25-179" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb25-180"><a href="#cb25-180" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Output:} \\</span></span>
<span id="cb25-181"><a href="#cb25-181" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ $\boldsymbol{\beta}$ the vector of coefficients \\</span></span>
<span id="cb25-182"><a href="#cb25-182" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb25-183"><a href="#cb25-183" aria-hidden="true" tabindex="-1"></a><span class="in">  \If{$G_K = \emptyset$}</span></span>
<span id="cb25-184"><a href="#cb25-184" aria-hidden="true" tabindex="-1"></a><span class="in">    \Comment{No prior information is known}</span></span>
<span id="cb25-185"><a href="#cb25-185" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $\boldsymbol \beta$ obtained with classical LASSO \\</span></span>
<span id="cb25-186"><a href="#cb25-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-187"><a href="#cb25-187" aria-hidden="true" tabindex="-1"></a><span class="in">  \ElsIf{$G_U = \emptyset$ \And $|G_K| \leq n$ }</span></span>
<span id="cb25-188"><a href="#cb25-188" aria-hidden="true" tabindex="-1"></a><span class="in">    \Comment{All regressors are known}</span></span>
<span id="cb25-189"><a href="#cb25-189" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $\boldsymbol \beta$ obtained with classical regression \\</span></span>
<span id="cb25-190"><a href="#cb25-190" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb25-191"><a href="#cb25-191" aria-hidden="true" tabindex="-1"></a><span class="in">  \Else</span></span>
<span id="cb25-192"><a href="#cb25-192" aria-hidden="true" tabindex="-1"></a><span class="in">  \State compute $\mathbf x_K$ and $\mathbf x_U$ from $\mathbf x$ </span></span>
<span id="cb25-193"><a href="#cb25-193" aria-hidden="true" tabindex="-1"></a><span class="in">  \State add a column of $1$ in $\mathbf x_K$ for the intercept estimation </span></span>
<span id="cb25-194"><a href="#cb25-194" aria-hidden="true" tabindex="-1"></a><span class="in">  \State compute $\mathbf u = (I_n-\mathbf x_K(\mathbf x_K^t\mathbf x_K)^{-1}\mathbf x_K^t)\mathbf y$ and     $ \mathbf v = \mathbf x_{U}-\mathbf x_K(\mathbf x_K^T\mathbf x_K)^{-1}\mathbf x_K^t\mathbf x_{U}$</span></span>
<span id="cb25-195"><a href="#cb25-195" aria-hidden="true" tabindex="-1"></a><span class="in">  \State  solve $\min_{\boldsymbol \beta_U} \big( \frac{1}{2n}|| \mathbf u - \boldsymbol         \beta_U\mathbf v||^2_2,+ \lambda \sum_{j,\,X_j \in G_U}\,|\beta_j| \big)$ with glmnet to obtain $\widehat{\boldsymbol\beta_U}$ </span></span>
<span id="cb25-196"><a href="#cb25-196" aria-hidden="true" tabindex="-1"></a><span class="in">  \State deduce $\widehat{\boldsymbol\beta_K}$ </span></span>
<span id="cb25-197"><a href="#cb25-197" aria-hidden="true" tabindex="-1"></a><span class="in">  \EndIf</span></span>
<span id="cb25-198"><a href="#cb25-198" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb25-199"><a href="#cb25-199" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb25-200"><a href="#cb25-200" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-201"><a href="#cb25-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-202"><a href="#cb25-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-203"><a href="#cb25-203" aria-hidden="true" tabindex="-1"></a><span class="fu"># Numerical experiments {#simulations}</span></span>
<span id="cb25-204"><a href="#cb25-204" aria-hidden="true" tabindex="-1"></a>To compare our method of parameters estimation with the one of 0-1 weighted LASSO, we perform numerical simulations. We focus on simulations where the number of observations $n=100$ and the number of covariates $p =500$. We also choose to run simulations where the explanatory variables $(X_1,\dots,X_p)$ are correlated, to put our algorithm in a more challenging situation. Finally, only $p_u = 40$ variables are actually used to build the model which means only $40$ coordinates of $\boldsymbol \beta \in \mathbb{R}^p$ are really used to construct $Y$.</span>
<span id="cb25-205"><a href="#cb25-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-206"><a href="#cb25-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-207"><a href="#cb25-207" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data simulation {#data-simu}</span></span>
<span id="cb25-208"><a href="#cb25-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-209"><a href="#cb25-209" aria-hidden="true" tabindex="-1"></a>To simulate correlated data in a realistic way, we rely on the work by @friguet2010impact. We first divide the $p$ variables in $H$ independent groups. For each group $h$, a covariance matrix is computed using gaussian latent variables to determine correlations between variables, and a dataset of size $n \times \frac{p}{H}$ is sampled from a multivariate normal distribution. The data generated for each group $h \in <span class="co">[</span><span class="ot">1,H</span><span class="co">]</span>$ are then merged together in a single dataset. The structure of the global covariance matrix of the data is thus block diagonal. An excerpt of this matrix is presented @fig-cor as an example. The code to generate the data, based on page $123$ of @friguet2010impact, is given below, the parameter $H$ being set to $5$.</span>
<span id="cb25-210"><a href="#cb25-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-211"><a href="#cb25-211" aria-hidden="true" tabindex="-1"></a><span class="in">```{r , echo=TRUE}</span></span>
<span id="cb25-212"><a href="#cb25-212" aria-hidden="true" tabindex="-1"></a><span class="in">library(mvtnorm)</span></span>
<span id="cb25-213"><a href="#cb25-213" aria-hidden="true" tabindex="-1"></a><span class="in">library(corrplot,quietly = TRUE)</span></span>
<span id="cb25-214"><a href="#cb25-214" aria-hidden="true" tabindex="-1"></a><span class="in">p = 500</span></span>
<span id="cb25-215"><a href="#cb25-215" aria-hidden="true" tabindex="-1"></a><span class="in">n = 100</span></span>
<span id="cb25-216"><a href="#cb25-216" aria-hidden="true" tabindex="-1"></a><span class="in">H = 5</span></span>
<span id="cb25-217"><a href="#cb25-217" aria-hidden="true" tabindex="-1"></a><span class="in">constant = 10 # intercept</span></span>
<span id="cb25-218"><a href="#cb25-218" aria-hidden="true" tabindex="-1"></a><span class="in">q = rep(4,H) # number of latent variables in each group</span></span>
<span id="cb25-219"><a href="#cb25-219" aria-hidden="true" tabindex="-1"></a><span class="in">strength = 7/10 # level of the correlation between variables of the same group</span></span>
<span id="cb25-220"><a href="#cb25-220" aria-hidden="true" tabindex="-1"></a><span class="in">data = matrix(rep(0, n*p), ncol = p, nrow = n)</span></span>
<span id="cb25-221"><a href="#cb25-221" aria-hidden="true" tabindex="-1"></a><span class="in">for (h in 1:H){</span></span>
<span id="cb25-222"><a href="#cb25-222" aria-hidden="true" tabindex="-1"></a><span class="in">  B_k = strength*rmvnorm(p/H, mean = rep(0, q[h]), sigma = diag(q[h]))</span></span>
<span id="cb25-223"><a href="#cb25-223" aria-hidden="true" tabindex="-1"></a><span class="in">  Sigma_k = apply(B_k^2, 1, sum) + rep(1, p/H)</span></span>
<span id="cb25-224"><a href="#cb25-224" aria-hidden="true" tabindex="-1"></a><span class="in">  B_k = diag(1/sqrt(Sigma_k))%*%B_k</span></span>
<span id="cb25-225"><a href="#cb25-225" aria-hidden="true" tabindex="-1"></a><span class="in">  Psi_k = diag(1/Sigma_k)</span></span>
<span id="cb25-226"><a href="#cb25-226" aria-hidden="true" tabindex="-1"></a><span class="in">  # matrix of var-covar</span></span>
<span id="cb25-227"><a href="#cb25-227" aria-hidden="true" tabindex="-1"></a><span class="in">  Var_k = B_k %*% t(B_k) + Psi_k</span></span>
<span id="cb25-228"><a href="#cb25-228" aria-hidden="true" tabindex="-1"></a><span class="in">  # data generation</span></span>
<span id="cb25-229"><a href="#cb25-229" aria-hidden="true" tabindex="-1"></a><span class="in">  X_k = rmvnorm (n, mean = rep(0,p/H), sigma = Var_k) # mean 0</span></span>
<span id="cb25-230"><a href="#cb25-230" aria-hidden="true" tabindex="-1"></a><span class="in">  data[,((h-1)*p/H+1):(h*p/H)] = X_k </span></span>
<span id="cb25-231"><a href="#cb25-231" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb25-232"><a href="#cb25-232" aria-hidden="true" tabindex="-1"></a><span class="in">data = as.data.frame(data)</span></span>
<span id="cb25-233"><a href="#cb25-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-234"><a href="#cb25-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-237"><a href="#cb25-237" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-238"><a href="#cb25-238" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cor</span></span>
<span id="cb25-239"><a href="#cb25-239" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Correlation matrix of the variables of the model. Only the first $20$ variables of each group are plotted."</span></span>
<span id="cb25-240"><a href="#cb25-240" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb25-241"><a href="#cb25-241" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"data/V_k_same.RData"</span>) <span class="co"># final matrix of var covar</span></span>
<span id="cb25-242"><a href="#cb25-242" aria-hidden="true" tabindex="-1"></a>Var_covar <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">*</span>p),<span class="at">ncol=</span>p,<span class="at">nrow =</span> p)</span>
<span id="cb25-243"><a href="#cb25-243" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (h <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb25-244"><a href="#cb25-244" aria-hidden="true" tabindex="-1"></a>  Var_covar[((h<span class="dv">-1</span>)<span class="sc">*</span>p<span class="sc">/</span>H<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(h<span class="sc">*</span>p<span class="sc">/</span>H),((h<span class="dv">-1</span>)<span class="sc">*</span>p<span class="sc">/</span>H<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(h<span class="sc">*</span>p<span class="sc">/</span>H)] <span class="ot">=</span> Var_list[[h]]</span>
<span id="cb25-245"><a href="#cb25-245" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-246"><a href="#cb25-246" aria-hidden="true" tabindex="-1"></a><span class="fu">corrplot</span>(Var_covar[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,<span class="dv">101</span><span class="sc">:</span><span class="dv">120</span>,<span class="dv">201</span><span class="sc">:</span><span class="dv">220</span>,<span class="dv">301</span><span class="sc">:</span><span class="dv">320</span>,<span class="dv">401</span><span class="sc">:</span><span class="dv">420</span>),<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,<span class="dv">101</span><span class="sc">:</span><span class="dv">120</span>,<span class="dv">201</span><span class="sc">:</span><span class="dv">220</span>,<span class="dv">301</span><span class="sc">:</span><span class="dv">320</span>,<span class="dv">401</span><span class="sc">:</span><span class="dv">420</span>)],<span class="at">tl.pos =</span> <span class="st">'n'</span>,<span class="at">addgrid.col=</span><span class="cn">NA</span>)</span>
<span id="cb25-247"><a href="#cb25-247" aria-hidden="true" tabindex="-1"></a> <span class="co">#c = corrplot(cor(data[,c(1:20,101:120,201:220,301:320,401:420)]),tl.pos = 'n',addgrid.col=NA) empiric matrix</span></span>
<span id="cb25-248"><a href="#cb25-248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-249"><a href="#cb25-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-250"><a href="#cb25-250" aria-hidden="true" tabindex="-1"></a>Once we got the data matrix $\boldsymbol x$, we compute $\mathbf y$ using the $8$ first variables of each group. The coefficients $\beta_j$ which are not $0$ go from $1$ to $8$ in each group. Our final model for the numerical simulations is thus: </span>
<span id="cb25-251"><a href="#cb25-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-252"><a href="#cb25-252" aria-hidden="true" tabindex="-1"></a>y = \beta_0 + \sum_{j=1}^8 \beta_jx_j  +  \sum_{j=101}^{108} \beta_jx_j +  \sum_{j=201}^{208} \beta_jx_j + \sum_{j=301}^{308} \beta_jx_j + \sum_{j=401}^{408} \beta_jx_j + \epsilon \,,</span>
<span id="cb25-253"><a href="#cb25-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb25-254"><a href="#cb25-254" aria-hidden="true" tabindex="-1"></a>with $\beta_1 = \beta_{101} = \dots = \beta_{401} = 1, \quad \dots \quad,\beta_8 = \beta_{108} = \dots = \beta_{408} = 8$ and $\epsilon \sim \mathcal{N}(0,4)$.</span>
<span id="cb25-255"><a href="#cb25-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-256"><a href="#cb25-256" aria-hidden="true" tabindex="-1"></a><span class="in">```{r,echo=TRUE}</span></span>
<span id="cb25-257"><a href="#cb25-257" aria-hidden="true" tabindex="-1"></a><span class="in">library(expm,quietly = TRUE)</span></span>
<span id="cb25-258"><a href="#cb25-258" aria-hidden="true" tabindex="-1"></a><span class="in">standard_d = 2</span></span>
<span id="cb25-259"><a href="#cb25-259" aria-hidden="true" tabindex="-1"></a><span class="in">beta = rep(c(1:8,rep(0,92)),5)</span></span>
<span id="cb25-260"><a href="#cb25-260" aria-hidden="true" tabindex="-1"></a><span class="in">Y = as.matrix(data)%*%beta + rnorm(n, mean = 0, sd = standard_d) + rep(constant, n)</span></span>
<span id="cb25-261"><a href="#cb25-261" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(Y) = "Y"</span></span>
<span id="cb25-262"><a href="#cb25-262" aria-hidden="true" tabindex="-1"></a><span class="in">data = cbind(data,Y)</span></span>
<span id="cb25-263"><a href="#cb25-263" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-264"><a href="#cb25-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-265"><a href="#cb25-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-266"><a href="#cb25-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-267"><a href="#cb25-267" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction of a priori knowledge</span></span>
<span id="cb25-268"><a href="#cb25-268" aria-hidden="true" tabindex="-1"></a> To see how the semi-LASSO algorithm performs with different levels of prior knowledge, we progressively add variables from the $G_U$ group to the $G_K$ group. $11$ scenarii are tested, starting with with $G_K = \emptyset$ where no regressor is known. We then add $4$ variables (corresponding to $10\%$ of true regressors) in $G_K$ several times to finally reach $G_K = (X_{j_1},\dots,X_{j_{p_u}})$ where $X_{j_1},\dots,X_{j_{p_u}}$ are the $p_u$ variables used to build the model $(j_1 = 1,\dots,j_8=8,j_9=101,...,j_{40}=408)$, as explained in @tbl-scenario. One variable of each group $h \in <span class="co">[</span><span class="ot">1,H-1</span><span class="co">]</span>$ is added in scenarii $s_1$ to $s_8$, then four variables of the last group $h=H$ are added in $s_9$ and $s_{10}$. In scenario $s_{10}$, all $p_u$ variables have been added to $G_K$.</span>
<span id="cb25-269"><a href="#cb25-269" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb25-270"><a href="#cb25-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-271"><a href="#cb25-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-272"><a href="#cb25-272" aria-hidden="true" tabindex="-1"></a>| Scenario  |    % of prior knowledge   |   Variables in $G_K$|  Variables in $G_U$|</span>
<span id="cb25-273"><a href="#cb25-273" aria-hidden="true" tabindex="-1"></a>|:----------:|:-------------:|:-------:|:-------:|</span>
<span id="cb25-274"><a href="#cb25-274" aria-hidden="true" tabindex="-1"></a>| $s_0$ | 0  | 0 | 500|</span>
<span id="cb25-275"><a href="#cb25-275" aria-hidden="true" tabindex="-1"></a>| $s_1$ |   10   |   4 | 496 |</span>
<span id="cb25-276"><a href="#cb25-276" aria-hidden="true" tabindex="-1"></a>| $s_2$ | 20 |    8 | 492 |</span>
<span id="cb25-277"><a href="#cb25-277" aria-hidden="true" tabindex="-1"></a>| $s_3$ | 30 |    12 | 488 |</span>
<span id="cb25-278"><a href="#cb25-278" aria-hidden="true" tabindex="-1"></a>| $s_4$ | 40 |    16 | 484 |</span>
<span id="cb25-279"><a href="#cb25-279" aria-hidden="true" tabindex="-1"></a>| $s_5$ | 50 |    20 | 480|</span>
<span id="cb25-280"><a href="#cb25-280" aria-hidden="true" tabindex="-1"></a>| $s_6$ | 60 |    24 | 476|</span>
<span id="cb25-281"><a href="#cb25-281" aria-hidden="true" tabindex="-1"></a>| $s_7$ | 70|    28 | 472 |</span>
<span id="cb25-282"><a href="#cb25-282" aria-hidden="true" tabindex="-1"></a>| $s_8$ | 80 |    32 | 468|</span>
<span id="cb25-283"><a href="#cb25-283" aria-hidden="true" tabindex="-1"></a>| $s_9$ | 90 |    36 | 464|</span>
<span id="cb25-284"><a href="#cb25-284" aria-hidden="true" tabindex="-1"></a>| $s_{10}$ | 100 |    40 | 460|</span>
<span id="cb25-285"><a href="#cb25-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-286"><a href="#cb25-286" aria-hidden="true" tabindex="-1"></a>: Description of each scenario to estimate the $\boldsymbol \beta$ coefficients {#tbl-scenario}</span>
<span id="cb25-287"><a href="#cb25-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-288"><a href="#cb25-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-289"><a href="#cb25-289" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb25-290"><a href="#cb25-290" aria-hidden="true" tabindex="-1"></a><span class="in"># Construction of the vector of prior knowledge used to progressively add variables in G_K</span></span>
<span id="cb25-291"><a href="#cb25-291" aria-hidden="true" tabindex="-1"></a><span class="in">prior_know = c()</span></span>
<span id="cb25-292"><a href="#cb25-292" aria-hidden="true" tabindex="-1"></a><span class="in">for (h in 1:(H-1)){</span></span>
<span id="cb25-293"><a href="#cb25-293" aria-hidden="true" tabindex="-1"></a><span class="in">  # for the first 4 groups, the first 8 variables will be progressively added to G_K </span></span>
<span id="cb25-294"><a href="#cb25-294" aria-hidden="true" tabindex="-1"></a><span class="in">  # (corresponding to scenarii s_1 to s_8)</span></span>
<span id="cb25-295"><a href="#cb25-295" aria-hidden="true" tabindex="-1"></a><span class="in">  prior_know_h = c(1/10*1:8, rep("not used",92)) </span></span>
<span id="cb25-296"><a href="#cb25-296" aria-hidden="true" tabindex="-1"></a><span class="in">  # for the variables not used in the model, prior_know is set to `not used`. </span></span>
<span id="cb25-297"><a href="#cb25-297" aria-hidden="true" tabindex="-1"></a><span class="in">  # It means they will never be included as prior knowledge in the model</span></span>
<span id="cb25-298"><a href="#cb25-298" aria-hidden="true" tabindex="-1"></a><span class="in">  prior_know = c(prior_know, prior_know_h)</span></span>
<span id="cb25-299"><a href="#cb25-299" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb25-300"><a href="#cb25-300" aria-hidden="true" tabindex="-1"></a><span class="in"># Concerning the last group (h=5), the first 8 variables will be added in s_9 and s_10</span></span>
<span id="cb25-301"><a href="#cb25-301" aria-hidden="true" tabindex="-1"></a><span class="in">prior_know = c(prior_know, rep(0.9,4),rep(1,4), rep("not used",92))</span></span>
<span id="cb25-302"><a href="#cb25-302" aria-hidden="true" tabindex="-1"></a><span class="in">names(prior_know) = colnames(data[,-dim(data)[2]])</span></span>
<span id="cb25-303"><a href="#cb25-303" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-304"><a href="#cb25-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-305"><a href="#cb25-305" aria-hidden="true" tabindex="-1"></a>The vector of prior knowledge is then used to iteratively construct the groups $G_K$ and $G_U$ for each scenario :</span>
<span id="cb25-306"><a href="#cb25-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-307"><a href="#cb25-307" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb25-308"><a href="#cb25-308" aria-hidden="true" tabindex="-1"></a><span class="in">Groups_construction &lt;- function (prior,partial_know){</span></span>
<span id="cb25-309"><a href="#cb25-309" aria-hidden="true" tabindex="-1"></a><span class="in">  # prior : vector of the prior knowledge indicating when the variable enters into the G_K group</span></span>
<span id="cb25-310"><a href="#cb25-310" aria-hidden="true" tabindex="-1"></a><span class="in">  # partial_know : proportion of partial knowledge to include (between 0 and 1)</span></span>
<span id="cb25-311"><a href="#cb25-311" aria-hidden="true" tabindex="-1"></a><span class="in">  G_K = c()</span></span>
<span id="cb25-312"><a href="#cb25-312" aria-hidden="true" tabindex="-1"></a><span class="in">  G_U = c()</span></span>
<span id="cb25-313"><a href="#cb25-313" aria-hidden="true" tabindex="-1"></a><span class="in">  # G_K = group with a priori</span></span>
<span id="cb25-314"><a href="#cb25-314" aria-hidden="true" tabindex="-1"></a><span class="in">  # G_U = group with no a priori</span></span>
<span id="cb25-315"><a href="#cb25-315" aria-hidden="true" tabindex="-1"></a><span class="in">  for (j in 1:length(prior)){</span></span>
<span id="cb25-316"><a href="#cb25-316" aria-hidden="true" tabindex="-1"></a><span class="in">    if((prior[j]&lt;= partial_know)){ </span></span>
<span id="cb25-317"><a href="#cb25-317" aria-hidden="true" tabindex="-1"></a><span class="in">      G_K = c(G_K, names(prior)[j])</span></span>
<span id="cb25-318"><a href="#cb25-318" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb25-319"><a href="#cb25-319" aria-hidden="true" tabindex="-1"></a><span class="in">    else{</span></span>
<span id="cb25-320"><a href="#cb25-320" aria-hidden="true" tabindex="-1"></a><span class="in">        G_U = c(G_U, names(prior)[j])</span></span>
<span id="cb25-321"><a href="#cb25-321" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb25-322"><a href="#cb25-322" aria-hidden="true" tabindex="-1"></a><span class="in">  }</span></span>
<span id="cb25-323"><a href="#cb25-323" aria-hidden="true" tabindex="-1"></a><span class="in">  list(G_K, G_U)</span></span>
<span id="cb25-324"><a href="#cb25-324" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb25-325"><a href="#cb25-325" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-326"><a href="#cb25-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-327"><a href="#cb25-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-328"><a href="#cb25-328" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimation of the parameters</span></span>
<span id="cb25-329"><a href="#cb25-329" aria-hidden="true" tabindex="-1"></a>To select an appropriate value for the parameter $\lambda$, we use, both in semi-LASSO and 0-1 weighted LASSO methods, the <span class="in">`cv.glmnet`</span> function which performs 10-fold cross-validation. $K=50$ datasets are generated following the scheme of Section <span class="co">[</span><span class="ot">4.1</span><span class="co">](#data-simu)</span>. A parameters estimation is performed for each simulation in order to evaluate the variability coming both from the noise $\epsilon$ in @eq-regression at the step of data generation and the cross-validation in <span class="in">`cv.glmnet`</span> . We use both semi-LASSO and 0-1 weighted LASSO to estimate $\boldsymbol \beta$ given a dataset and a scenario of @tbl-scenario. All the results presented in Section <span class="co">[</span><span class="ot">5</span><span class="co">](#results)</span> are shown as boxplots relative to these $50$ replica. The code below shows how the parameters estimation is performed for a particular dataset.</span>
<span id="cb25-332"><a href="#cb25-332" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-333"><a href="#cb25-333" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(attempt)</span>
<span id="cb25-334"><a href="#cb25-334" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb25-335"><a href="#cb25-335" aria-hidden="true" tabindex="-1"></a>semi_LASSO <span class="ot">&lt;-</span> <span class="cf">function</span>(known_group, unknown_group, data_transform, response_var, <span class="at">inter =</span> <span class="cn">TRUE</span>){</span>
<span id="cb25-336"><a href="#cb25-336" aria-hidden="true" tabindex="-1"></a>  <span class="co"># known and unknown groupe give the indices of variables for which we have (or not) an a priori</span></span>
<span id="cb25-337"><a href="#cb25-337" aria-hidden="true" tabindex="-1"></a>  <span class="co"># data_transform is the dataset containing all covariates X and the response</span></span>
<span id="cb25-338"><a href="#cb25-338" aria-hidden="true" tabindex="-1"></a>  <span class="co"># variable Y</span></span>
<span id="cb25-339"><a href="#cb25-339" aria-hidden="true" tabindex="-1"></a>  <span class="co"># response_var gives the name of the variable in the dataset to be considered</span></span>
<span id="cb25-340"><a href="#cb25-340" aria-hidden="true" tabindex="-1"></a>  <span class="co"># as the response variable</span></span>
<span id="cb25-341"><a href="#cb25-341" aria-hidden="true" tabindex="-1"></a>  <span class="co"># inter is TRUE or FALSE to include or not an intercept into the model</span></span>
<span id="cb25-342"><a href="#cb25-342" aria-hidden="true" tabindex="-1"></a>  <span class="fu">warn_if</span>(response_var <span class="sc">%in%</span> known_group <span class="sc">||</span>response_var <span class="sc">%in%</span> unknown_group  ,isTRUE,</span>
<span id="cb25-343"><a href="#cb25-343" aria-hidden="true" tabindex="-1"></a>          <span class="at">msg =</span> <span class="st">"Your response variable will be used as an explicative variable. </span></span>
<span id="cb25-344"><a href="#cb25-344" aria-hidden="true" tabindex="-1"></a><span class="st">          Maybe you put it in one of the two groups"</span>)</span>
<span id="cb25-345"><a href="#cb25-345" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Warn if the response variable is put in G_K or G_U</span></span>
<span id="cb25-346"><a href="#cb25-346" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">dim</span>(data_transform)[<span class="dv">1</span>]</span>
<span id="cb25-347"><a href="#cb25-347" aria-hidden="true" tabindex="-1"></a>  p<span class="ot">=</span> <span class="fu">dim</span>(data_transform)[<span class="dv">2</span>]<span class="sc">-</span><span class="dv">1</span> <span class="co"># remove the response variable</span></span>
<span id="cb25-348"><a href="#cb25-348" aria-hidden="true" tabindex="-1"></a>  G_K <span class="ot">=</span> known_group</span>
<span id="cb25-349"><a href="#cb25-349" aria-hidden="true" tabindex="-1"></a>  G_U <span class="ot">=</span> unknown_group</span>
<span id="cb25-350"><a href="#cb25-350" aria-hidden="true" tabindex="-1"></a>  ind <span class="ot">=</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data_transform)<span class="sc">==</span>response_var)</span>
<span id="cb25-351"><a href="#cb25-351" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb25-352"><a href="#cb25-352" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(G_K)<span class="sc">==</span><span class="dv">0</span>){</span>
<span id="cb25-353"><a href="#cb25-353" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Case when there is no a priori.</span></span>
<span id="cb25-354"><a href="#cb25-354" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Then it's just a classic LASSO</span></span>
<span id="cb25-355"><a href="#cb25-355" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">=</span> <span class="fu">cv.glmnet</span>(<span class="fu">as.matrix</span>(data_transform[,<span class="sc">-</span>ind]),<span class="fu">as.matrix</span>(data_transform[,ind]),<span class="at">alpha =</span> <span class="dv">1</span>,<span class="at">grouped=</span><span class="cn">FALSE</span>, <span class="at">intercept =</span> inter)</span>
<span id="cb25-356"><a href="#cb25-356" aria-hidden="true" tabindex="-1"></a>    lambda <span class="ot">=</span> model<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb25-357"><a href="#cb25-357" aria-hidden="true" tabindex="-1"></a>    coeff <span class="ot">=</span> <span class="fu">coef</span>(model)</span>
<span id="cb25-358"><a href="#cb25-358" aria-hidden="true" tabindex="-1"></a>    beta_NK <span class="ot">=</span> coeff[<span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(coeff)]</span>
<span id="cb25-359"><a href="#cb25-359" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb25-360"><a href="#cb25-360" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(beta) <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">colnames</span>(data_transform[,<span class="sc">-</span>ind]),<span class="st">"Intercept"</span>)</span>
<span id="cb25-361"><a href="#cb25-361" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(G_U)){</span>
<span id="cb25-362"><a href="#cb25-362" aria-hidden="true" tabindex="-1"></a>      beta[G_U[j]]<span class="ot">=</span> beta_NK[j]</span>
<span id="cb25-363"><a href="#cb25-363" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb25-364"><a href="#cb25-364" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we add the value of intercept</span></span>
<span id="cb25-365"><a href="#cb25-365" aria-hidden="true" tabindex="-1"></a>    beta[p<span class="sc">+</span><span class="dv">1</span>] <span class="ot">=</span> coeff[<span class="dv">1</span>]</span>
<span id="cb25-366"><a href="#cb25-366" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb25-367"><a href="#cb25-367" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(G_U)<span class="sc">==</span><span class="dv">0</span>){</span>
<span id="cb25-368"><a href="#cb25-368" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Case when all regressors are known</span></span>
<span id="cb25-369"><a href="#cb25-369" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Then it's a classic regression (we suppose that the number of regressors </span></span>
<span id="cb25-370"><a href="#cb25-370" aria-hidden="true" tabindex="-1"></a>    <span class="co"># will not exceed the number of observations)</span></span>
<span id="cb25-371"><a href="#cb25-371" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-372"><a href="#cb25-372" aria-hidden="true" tabindex="-1"></a>    list_names_data <span class="ot">=</span> <span class="fu">colnames</span>(data_transform)[G_K]</span>
<span id="cb25-373"><a href="#cb25-373" aria-hidden="true" tabindex="-1"></a>    lambda <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb25-374"><a href="#cb25-374" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-375"><a href="#cb25-375" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">length</span>(list_names_data)<span class="sc">!=</span><span class="dv">0</span>){</span>
<span id="cb25-376"><a href="#cb25-376" aria-hidden="true" tabindex="-1"></a>      reg_j <span class="ot">=</span> <span class="fu">lm</span>(<span class="at">formula =</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(response_var, <span class="st">"~ 0+"</span>, <span class="fu">paste</span>(list_names_data, <span class="at">collapse =</span> <span class="st">"+"</span>))), <span class="at">data =</span> data_transform)</span>
<span id="cb25-377"><a href="#cb25-377" aria-hidden="true" tabindex="-1"></a>      resume_j <span class="ot">=</span> reg_j<span class="sc">$</span>coefficients</span>
<span id="cb25-378"><a href="#cb25-378" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb25-379"><a href="#cb25-379" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> {</span>
<span id="cb25-380"><a href="#cb25-380" aria-hidden="true" tabindex="-1"></a>      <span class="co"># The case when j is an initial node with no parent</span></span>
<span id="cb25-381"><a href="#cb25-381" aria-hidden="true" tabindex="-1"></a>      resume_j <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,p)</span>
<span id="cb25-382"><a href="#cb25-382" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb25-383"><a href="#cb25-383" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-384"><a href="#cb25-384" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-385"><a href="#cb25-385" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creation of the final vector of coefficients</span></span>
<span id="cb25-386"><a href="#cb25-386" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb25-387"><a href="#cb25-387" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(beta) <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">colnames</span>(data_transform[,<span class="sc">-</span>ind]),<span class="st">"Intercept"</span>)</span>
<span id="cb25-388"><a href="#cb25-388" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(G_K)){</span>
<span id="cb25-389"><a href="#cb25-389" aria-hidden="true" tabindex="-1"></a>      beta[G_U[j]]<span class="ot">=</span> resume_j[j]</span>
<span id="cb25-390"><a href="#cb25-390" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb25-391"><a href="#cb25-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-392"><a href="#cb25-392" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-393"><a href="#cb25-393" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb25-394"><a href="#cb25-394" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> ((<span class="fu">length</span>(G_U)<span class="sc">!=</span><span class="dv">0</span>)<span class="sc">&amp;</span>(<span class="fu">length</span>(G_K)<span class="sc">!=</span><span class="dv">0</span>)){</span>
<span id="cb25-395"><a href="#cb25-395" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Semi-LASSO</span></span>
<span id="cb25-396"><a href="#cb25-396" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creation of all necessary objects X_NK and X_K</span></span>
<span id="cb25-397"><a href="#cb25-397" aria-hidden="true" tabindex="-1"></a>    X_NK <span class="ot">=</span><span class="fu">as.matrix</span>(data_transform[,G_U]) </span>
<span id="cb25-398"><a href="#cb25-398" aria-hidden="true" tabindex="-1"></a>    X_K <span class="ot">=</span> <span class="fu">cbind</span>(<span class="fu">as.matrix</span>(data_transform[,G_K]),<span class="fu">rep</span>(<span class="dv">1</span>,n)) <span class="co"># we add a column for the intercept</span></span>
<span id="cb25-399"><a href="#cb25-399" aria-hidden="true" tabindex="-1"></a>    Y <span class="ot">=</span> <span class="fu">as.matrix</span>(data_transform[,ind])</span>
<span id="cb25-400"><a href="#cb25-400" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creation of U and V</span></span>
<span id="cb25-401"><a href="#cb25-401" aria-hidden="true" tabindex="-1"></a>    I_n <span class="ot">=</span> <span class="fu">diag</span>(n)</span>
<span id="cb25-402"><a href="#cb25-402" aria-hidden="true" tabindex="-1"></a>    U <span class="ot">=</span> (I_n <span class="sc">-</span> X_K<span class="sc">%*%</span><span class="fu">solve</span>(<span class="fu">t</span>(X_K)<span class="sc">%*%</span>X_K)<span class="sc">%*%</span><span class="fu">t</span>(X_K))<span class="sc">%*%</span>Y  <span class="co"># new Y</span></span>
<span id="cb25-403"><a href="#cb25-403" aria-hidden="true" tabindex="-1"></a>    V <span class="ot">=</span> X_NK <span class="sc">-</span> X_K<span class="sc">%*%</span><span class="fu">solve</span>(<span class="fu">t</span>(X_K)<span class="sc">%*%</span>X_K)<span class="sc">%*%</span><span class="fu">t</span>(X_K)<span class="sc">%*%</span>X_NK</span>
<span id="cb25-404"><a href="#cb25-404" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Performing first step LASSO, to find beta_NK</span></span>
<span id="cb25-405"><a href="#cb25-405" aria-hidden="true" tabindex="-1"></a>    model_NK <span class="ot">=</span> <span class="fu">cv.glmnet</span>(V,U,<span class="at">alpha =</span> <span class="dv">1</span>,<span class="at">grouped=</span><span class="cn">FALSE</span>, <span class="at">intercept =</span> inter)</span>
<span id="cb25-406"><a href="#cb25-406" aria-hidden="true" tabindex="-1"></a>    lambda <span class="ot">=</span> model_NK<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb25-407"><a href="#cb25-407" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Getting the estimated coefficients and constructing beta_NK</span></span>
<span id="cb25-408"><a href="#cb25-408" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Be careful, the first parameter is the intercept in the coef() function</span></span>
<span id="cb25-409"><a href="#cb25-409" aria-hidden="true" tabindex="-1"></a>    coeff_NK <span class="ot">=</span> <span class="fu">coef</span>(model_NK)</span>
<span id="cb25-410"><a href="#cb25-410" aria-hidden="true" tabindex="-1"></a>    beta_NK <span class="ot">=</span> coeff_NK[<span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(coeff_NK)]</span>
<span id="cb25-411"><a href="#cb25-411" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-412"><a href="#cb25-412" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We can deduce beta_K from beta_NK</span></span>
<span id="cb25-413"><a href="#cb25-413" aria-hidden="true" tabindex="-1"></a>    beta_K <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X_K)<span class="sc">%*%</span>X_K)<span class="sc">%*%</span><span class="fu">t</span>(X_K)<span class="sc">%*%</span>(Y<span class="sc">-</span>X_NK<span class="sc">%*%</span>beta_NK)</span>
<span id="cb25-414"><a href="#cb25-414" aria-hidden="true" tabindex="-1"></a>    beta_K</span>
<span id="cb25-415"><a href="#cb25-415" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-416"><a href="#cb25-416" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Construction of the final beta</span></span>
<span id="cb25-417"><a href="#cb25-417" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb25-418"><a href="#cb25-418" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(beta) <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">colnames</span>(data_transform[,<span class="sc">-</span>ind]),<span class="st">"Intercept"</span>)</span>
<span id="cb25-419"><a href="#cb25-419" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(G_K)){</span>
<span id="cb25-420"><a href="#cb25-420" aria-hidden="true" tabindex="-1"></a>      beta[G_K[j]]<span class="ot">=</span> beta_K[j]</span>
<span id="cb25-421"><a href="#cb25-421" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb25-422"><a href="#cb25-422" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(G_U)){</span>
<span id="cb25-423"><a href="#cb25-423" aria-hidden="true" tabindex="-1"></a>      beta[G_U[j]]<span class="ot">=</span> beta_NK[j]</span>
<span id="cb25-424"><a href="#cb25-424" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb25-425"><a href="#cb25-425" aria-hidden="true" tabindex="-1"></a>    <span class="co">#beta[p+1] = coeff_NK[1]</span></span>
<span id="cb25-426"><a href="#cb25-426" aria-hidden="true" tabindex="-1"></a>    beta[p<span class="sc">+</span><span class="dv">1</span>] <span class="ot">=</span> beta_K[<span class="fu">length</span>(beta_K)]</span>
<span id="cb25-427"><a href="#cb25-427" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb25-428"><a href="#cb25-428" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(beta,lambda)</span>
<span id="cb25-429"><a href="#cb25-429" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-430"><a href="#cb25-430" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-431"><a href="#cb25-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-432"><a href="#cb25-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-433"><a href="#cb25-433" aria-hidden="true" tabindex="-1"></a><span class="in">```{r,echo=TRUE}</span></span>
<span id="cb25-434"><a href="#cb25-434" aria-hidden="true" tabindex="-1"></a><span class="in">load("data/Data.RData") </span></span>
<span id="cb25-435"><a href="#cb25-435" aria-hidden="true" tabindex="-1"></a><span class="in">options( "digits"=5, "scipen"=0) </span></span>
<span id="cb25-436"><a href="#cb25-436" aria-hidden="true" tabindex="-1"></a><span class="in"># loading of the list containing the K = 50 datasets</span></span>
<span id="cb25-437"><a href="#cb25-437" aria-hidden="true" tabindex="-1"></a><span class="in"># We show how the estimation works using only the first dataset</span></span>
<span id="cb25-438"><a href="#cb25-438" aria-hidden="true" tabindex="-1"></a><span class="in">data_K = Data[[1]]</span></span>
<span id="cb25-439"><a href="#cb25-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-440"><a href="#cb25-440" aria-hidden="true" tabindex="-1"></a><span class="in"># semi-LASSO</span></span>
<span id="cb25-441"><a href="#cb25-441" aria-hidden="true" tabindex="-1"></a><span class="in">regressors_list_by_knowledge = list() # creation of the list of regressors</span></span>
<span id="cb25-442"><a href="#cb25-442" aria-hidden="true" tabindex="-1"></a><span class="in"># each element of the list will correspond to a particular scenario</span></span>
<span id="cb25-443"><a href="#cb25-443" aria-hidden="true" tabindex="-1"></a><span class="in">index = 1</span></span>
<span id="cb25-444"><a href="#cb25-444" aria-hidden="true" tabindex="-1"></a><span class="in">for (l in seq(0, 1, by = 0.1)){ # levels of prior knowledge</span></span>
<span id="cb25-445"><a href="#cb25-445" aria-hidden="true" tabindex="-1"></a><span class="in">  A_priori_group = Groups_construction(prior_know,l) # construction of G_K and G_U</span></span>
<span id="cb25-446"><a href="#cb25-446" aria-hidden="true" tabindex="-1"></a><span class="in">  result = semi_LASSO(A_priori_group[[1]], A_priori_group[[2]], data_K,</span></span>
<span id="cb25-447"><a href="#cb25-447" aria-hidden="true" tabindex="-1"></a><span class="in">                      colnames(data_K)[length(colnames(data_K))],</span></span>
<span id="cb25-448"><a href="#cb25-448" aria-hidden="true" tabindex="-1"></a><span class="in">                      inter = TRUE)</span></span>
<span id="cb25-449"><a href="#cb25-449" aria-hidden="true" tabindex="-1"></a><span class="in">  regressors_list_by_knowledge[[index]] = result[[1]]</span></span>
<span id="cb25-450"><a href="#cb25-450" aria-hidden="true" tabindex="-1"></a><span class="in">  index = index + 1</span></span>
<span id="cb25-451"><a href="#cb25-451" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb25-452"><a href="#cb25-452" aria-hidden="true" tabindex="-1"></a><span class="in">names(regressors_list_by_knowledge) = seq(0, 1, by = 0.1)</span></span>
<span id="cb25-453"><a href="#cb25-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-454"><a href="#cb25-454" aria-hidden="true" tabindex="-1"></a><span class="in"># 0-1 weighted LASSO</span></span>
<span id="cb25-455"><a href="#cb25-455" aria-hidden="true" tabindex="-1"></a><span class="in">regressors_list_by_knowledge_glmnet = list()</span></span>
<span id="cb25-456"><a href="#cb25-456" aria-hidden="true" tabindex="-1"></a><span class="in">index = 1</span></span>
<span id="cb25-457"><a href="#cb25-457" aria-hidden="true" tabindex="-1"></a><span class="in">for (l in seq(0,1, by = 0.1)){</span></span>
<span id="cb25-458"><a href="#cb25-458" aria-hidden="true" tabindex="-1"></a><span class="in">  weights = as.numeric(prior_know&gt;l) # transformation of prior_know into 0-1 weights</span></span>
<span id="cb25-459"><a href="#cb25-459" aria-hidden="true" tabindex="-1"></a><span class="in">  # the p+1th variable in data_K is Y</span></span>
<span id="cb25-460"><a href="#cb25-460" aria-hidden="true" tabindex="-1"></a><span class="in">  reg = cv.glmnet(x = as.matrix(data_K[,-(p+1)]), y = as.matrix(data_K[,(p+1)]), penalty.factor = weights, grouped=FALSE, alpha=1, intercept=TRUE)</span></span>
<span id="cb25-461"><a href="#cb25-461" aria-hidden="true" tabindex="-1"></a><span class="in">  result = coef(reg)[-1,] # reorder the position of the intercept estimation</span></span>
<span id="cb25-462"><a href="#cb25-462" aria-hidden="true" tabindex="-1"></a><span class="in">  result = c(result, coef(reg)[1,])</span></span>
<span id="cb25-463"><a href="#cb25-463" aria-hidden="true" tabindex="-1"></a><span class="in">  names(result)[p+1] = "Intercept"</span></span>
<span id="cb25-464"><a href="#cb25-464" aria-hidden="true" tabindex="-1"></a><span class="in">  regressors_list_by_knowledge_glmnet[[index]] = result</span></span>
<span id="cb25-465"><a href="#cb25-465" aria-hidden="true" tabindex="-1"></a><span class="in">  index = index + 1</span></span>
<span id="cb25-466"><a href="#cb25-466" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb25-467"><a href="#cb25-467" aria-hidden="true" tabindex="-1"></a><span class="in">names(regressors_list_by_knowledge_glmnet)&lt;-seq(0, 1, by = 0.1)</span></span>
<span id="cb25-468"><a href="#cb25-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-469"><a href="#cb25-469" aria-hidden="true" tabindex="-1"></a><span class="in"># The first 8 estimated coefficients by semi-LASSO, for a prior knowledge of 20%</span></span>
<span id="cb25-470"><a href="#cb25-470" aria-hidden="true" tabindex="-1"></a><span class="in">regressors_list_by_knowledge[[3]][1:8]</span></span>
<span id="cb25-471"><a href="#cb25-471" aria-hidden="true" tabindex="-1"></a><span class="in"># The first 8 estimated coefficients by 0-1 weighted LASSO, for a prior knowledge of 20%</span></span>
<span id="cb25-472"><a href="#cb25-472" aria-hidden="true" tabindex="-1"></a><span class="in">regressors_list_by_knowledge_glmnet[[3]][1:8]</span></span>
<span id="cb25-473"><a href="#cb25-473" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-474"><a href="#cb25-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-475"><a href="#cb25-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-476"><a href="#cb25-476" aria-hidden="true" tabindex="-1"></a><span class="fu">## Criteria used to compare the methods {#criteria}</span></span>
<span id="cb25-477"><a href="#cb25-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-478"><a href="#cb25-478" aria-hidden="true" tabindex="-1"></a>To compare the results from semi-LASSO and the 0-1 weighted LASSO, we use the following indicators: </span>
<span id="cb25-479"><a href="#cb25-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-480"><a href="#cb25-480" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The sensitivity (true positive rate), defined as: $$\text{se} = \frac{TP}{TP + FN}= \frac{|<span class="sc">\{</span>\beta_j\neq0  <span class="sc">\}</span> \bigcap <span class="sc">\{</span>\widehat{\beta_j}\neq0 <span class="sc">\}</span>|}{|<span class="sc">\{</span>\beta_j\neq0  <span class="sc">\}</span>|},$$ with $\widehat{\beta_j}$ the estimated parameters of the model. TP is the number of true positives, i.e. the number of true regressors of the model that are correctly selected by the method, and FN is the number of false negatives, i.e the number of true regressors of the model that are not selected by the method. It measures the proportion of true regressors of the model correctly selected by the method among all the true regressors of the model .</span>
<span id="cb25-481"><a href="#cb25-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-482"><a href="#cb25-482" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The specificity (true negative rate), defined as: $$\text{sp} = \frac{TN}{TN+FP}\frac{|<span class="sc">\{</span>\beta_j=0  <span class="sc">\}</span> \bigcap <span class="sc">\{</span>\widehat{\beta_j}=0 <span class="sc">\}</span>|}{|<span class="sc">\{</span>\beta_j=0  <span class="sc">\}</span>|},$$ with TN the number of true negatives, i.e. the variables which are not regressors in the model and not found as such by the method, and FP the false positives, i.e. the variables which are not regressors in the model but found as such by the method. It measures the proportion of variables which are not regressors in the model and that the method correctly identify as such. </span>
<span id="cb25-483"><a href="#cb25-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-484"><a href="#cb25-484" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The Root Mean Square Error, defined as: $$\text{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2},$$ with $\hat{y}_i = \sum_{j=1}^{p}\widehat{\beta_i}x_i$ the prediction of $y_i$. It measures the capacity of prediction of the fitted model. It is calculated on a new dataset of size $n$, constructed with the procedure described in Section <span class="co">[</span><span class="ot">4.1</span><span class="co">](#data-simu)</span>. This dataset was not used to perform the parameters estimation.  </span>
<span id="cb25-485"><a href="#cb25-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-486"><a href="#cb25-486" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Individual coefficients estimation, defined as the gap between the estimated value $\widehat{\beta_j}$ and the true one $\beta_j$, to identify which method performs better in finding the true value of the coefficients, and the impact of the prior knowledge on the estimation.</span>
<span id="cb25-487"><a href="#cb25-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-488"><a href="#cb25-488" aria-hidden="true" tabindex="-1"></a>The first two criteria concern the ability of the fitted model to select the true regressors whereas the third one assesses the capacity to predict $Y$. As for the fourth one, it measures the quality of the estimation of a particular $\beta_j$. We also want to see the impact of the prior knowledge level on these indicators.</span>
<span id="cb25-489"><a href="#cb25-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-490"><a href="#cb25-490" aria-hidden="true" tabindex="-1"></a><span class="fu"># Results {#results}</span></span>
<span id="cb25-491"><a href="#cb25-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-492"><a href="#cb25-492" aria-hidden="true" tabindex="-1"></a>We present here the results of the simulations introduced in Section <span class="co">[</span><span class="ot">4</span><span class="co">](#simulations)</span>.</span>
<span id="cb25-493"><a href="#cb25-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-494"><a href="#cb25-494" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb25-495"><a href="#cb25-495" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggplot2)</span></span>
<span id="cb25-496"><a href="#cb25-496" aria-hidden="true" tabindex="-1"></a><span class="in">load("data/Data_final.RData") # load the RData containing</span></span>
<span id="cb25-497"><a href="#cb25-497" aria-hidden="true" tabindex="-1"></a><span class="in"># - parameters estimations for semi-LASSO and 0-1 weighted LASSO</span></span>
<span id="cb25-498"><a href="#cb25-498" aria-hidden="true" tabindex="-1"></a><span class="in"># - sensitivity</span></span>
<span id="cb25-499"><a href="#cb25-499" aria-hidden="true" tabindex="-1"></a><span class="in"># - specificity</span></span>
<span id="cb25-500"><a href="#cb25-500" aria-hidden="true" tabindex="-1"></a><span class="in"># - RMSE on the test dataset</span></span>
<span id="cb25-501"><a href="#cb25-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-502"><a href="#cb25-502" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-503"><a href="#cb25-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-504"><a href="#cb25-504" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sensitivity and specificity {#speci-sensi}</span></span>
<span id="cb25-505"><a href="#cb25-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-506"><a href="#cb25-506" aria-hidden="true" tabindex="-1"></a>We first give the results about the performance of variables selection, which is emphasized by the sensitivity and specificity described in Section <span class="co">[</span><span class="ot">4.4</span><span class="co">](#criteria)</span>.   </span>
<span id="cb25-507"><a href="#cb25-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-508"><a href="#cb25-508" aria-hidden="true" tabindex="-1"></a>@fig-sensi and @fig-speci present respectively the sensibility and the specifity for the semi-LASSO and 0-1 weighted LASSO methods. Each boxplot corresponds to the $50$ replica described in Section <span class="co">[</span><span class="ot">4.1</span><span class="co">](#data-simu)</span>, done for both methods (blue for 0-1 weighted LASSO, yellow for semi-LASSO) and a prior knowledge given in abscissa (see @tbl-scenario).   </span>
<span id="cb25-509"><a href="#cb25-509" aria-hidden="true" tabindex="-1"></a>We can first notice that in @fig-sensi the sensitivity increases with prior knowledge for both methods, which seems coherent with the fact that we force relevant variables to be included into the model. With a level of prior knowledge lower than $80\%$, the semi-LASSO method seems to be more efficient in finding true positive regressors. In the semi-LASSO method, we first remove the influence of the variables of $G_K$ before solving a LASSO problem on the remaining variables (see the expressions of $\mathbf u$ and $\mathbf v$ in Section <span class="co">[</span><span class="ot">3</span><span class="co">](#semilasso)</span>). This suggests that this additional step allows the final LASSO procedure to be more efficient in finding true regressors among the remaining variables. When the prior knowledge level is high enough ($&gt;90\%$), the given information is sufficient for both methods to perform equally well in terms of sensitivity.   </span>
<span id="cb25-510"><a href="#cb25-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-511"><a href="#cb25-511" aria-hidden="true" tabindex="-1"></a><span class="in">```{r,echo=TRUE}</span></span>
<span id="cb25-512"><a href="#cb25-512" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-sensi</span></span>
<span id="cb25-513"><a href="#cb25-513" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Boxplots of the sensitivity for each prior knowledge scenario and each method of parameters estimation."</span></span>
<span id="cb25-514"><a href="#cb25-514" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb25-515"><a href="#cb25-515" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(Data_final, aes(x = PriorKnowledge, y = Sensitivity)) + </span></span>
<span id="cb25-516"><a href="#cb25-516" aria-hidden="true" tabindex="-1"></a><span class="in">geom_boxplot(aes(fill = Method), position = position_dodge(0.9)) +</span></span>
<span id="cb25-517"><a href="#cb25-517" aria-hidden="true" tabindex="-1"></a><span class="in">scale_fill_manual(values = c("#00AFBB", "#E7B800")) +</span></span>
<span id="cb25-518"><a href="#cb25-518" aria-hidden="true" tabindex="-1"></a><span class="in">geom_hline(aes(yintercept = 1), color = "red") + expand_limits(y=c(0, 1)) +</span></span>
<span id="cb25-519"><a href="#cb25-519" aria-hidden="true" tabindex="-1"></a><span class="in">labs(x = "% of prior knowledge", y = "Sensitivity")</span></span>
<span id="cb25-520"><a href="#cb25-520" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-521"><a href="#cb25-521" aria-hidden="true" tabindex="-1"></a>Looking at the specificity in @fig-speci, we see that it is high for both methods and prior knowledge levels. The 0-1 weighted LASSO method seems to perform better when the prior knowledge level increases, and to limit the number of false positive into the model. The step of LASSO in the semi-LASSO algorithm is performed in a smaller space, and tries to find non-zero values for some coefficients. When the prior knowledge information increases, there are less and less correct regressors to find, which means the LASSO will include more and more irrelevant variables in the model. It is particularly true when the prior knowledge is set to $1$ and all correct regressors have been put in $G_K$. In the 0-1 weighted LASSO, the optimization is done directly in $\mathbb{R}^{p+1}$, which restricts the number of false positives. In both cases, the global specificity never goes below $0.85$, which is satisfying. </span>
<span id="cb25-522"><a href="#cb25-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-523"><a href="#cb25-523" aria-hidden="true" tabindex="-1"></a><span class="in">```{r,echo=TRUE}</span></span>
<span id="cb25-524"><a href="#cb25-524" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-speci</span></span>
<span id="cb25-525"><a href="#cb25-525" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Boxplots of the specificity for each prior knowledge scenario and each method of parameters estimation."</span></span>
<span id="cb25-526"><a href="#cb25-526" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb25-527"><a href="#cb25-527" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(Data_final, aes(x = PriorKnowledge, y = Specificity)) + </span></span>
<span id="cb25-528"><a href="#cb25-528" aria-hidden="true" tabindex="-1"></a><span class="in">geom_boxplot(aes(fill = Method), position = position_dodge(0.9)) +</span></span>
<span id="cb25-529"><a href="#cb25-529" aria-hidden="true" tabindex="-1"></a><span class="in">scale_fill_manual(values = c("#00AFBB", "#E7B800")) +</span></span>
<span id="cb25-530"><a href="#cb25-530" aria-hidden="true" tabindex="-1"></a><span class="in">geom_hline(aes(yintercept = 1),color = "red") +  expand_limits(y=c(0,1)) +</span></span>
<span id="cb25-531"><a href="#cb25-531" aria-hidden="true" tabindex="-1"></a><span class="in">labs(x = "% of prior knowledge", y = "Specificity")</span></span>
<span id="cb25-532"><a href="#cb25-532" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-533"><a href="#cb25-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-534"><a href="#cb25-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-535"><a href="#cb25-535" aria-hidden="true" tabindex="-1"></a><span class="fu">## RMSE</span></span>
<span id="cb25-536"><a href="#cb25-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-537"><a href="#cb25-537" aria-hidden="true" tabindex="-1"></a>Regarding the RMSE values plotted in @fig-rmse, we can see that the semi-LASSO has a lower prediction error than the 0-1 weighted LASSO for a prior knowledge level smaller than $80\%$. For higher values, the 2 methods are similar, with a slight advantage for the 0-1 weighted LASSO. This can be explained one more time by the irrelevant variables added into the model by the semi-LASSO method when the prior knowledge level is high.</span>
<span id="cb25-538"><a href="#cb25-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-539"><a href="#cb25-539" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb25-540"><a href="#cb25-540" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-rmse</span></span>
<span id="cb25-541"><a href="#cb25-541" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Boxplots of the RMSE calculated on a test dataset for each prior knowledge scenario and each method of parameters estimation."</span></span>
<span id="cb25-542"><a href="#cb25-542" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb25-543"><a href="#cb25-543" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(Data_final, aes(x = PriorKnowledge, y = RMSEtest)) + </span></span>
<span id="cb25-544"><a href="#cb25-544" aria-hidden="true" tabindex="-1"></a><span class="in">geom_boxplot(aes(fill = Method), position = position_dodge(0.9)) +</span></span>
<span id="cb25-545"><a href="#cb25-545" aria-hidden="true" tabindex="-1"></a><span class="in">scale_fill_manual(values = c("#00AFBB", "#E7B800")) +</span></span>
<span id="cb25-546"><a href="#cb25-546" aria-hidden="true" tabindex="-1"></a><span class="in">labs(x = "% of prior knowledge", y = "RMSE")</span></span>
<span id="cb25-547"><a href="#cb25-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-548"><a href="#cb25-548" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-549"><a href="#cb25-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-550"><a href="#cb25-550" aria-hidden="true" tabindex="-1"></a><span class="fu">## Parameters estimation</span></span>
<span id="cb25-551"><a href="#cb25-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-552"><a href="#cb25-552" aria-hidden="true" tabindex="-1"></a>To illustrate how the methods perform in estimating the parameters, we plot on @fig-beta-esti, the estimation of the parameter $\beta_7=7$, associated to the variable $X_7$. The horizontal red line indicates the true value of the coefficient, whereas the vertical one shows the first scenario where the variable $X_7$ is added to $G_K$ (see @tbl-scenario).   </span>
<span id="cb25-553"><a href="#cb25-553" aria-hidden="true" tabindex="-1"></a>Until the prior knowledge level reaches $70\%$, the semi-LASSO estimates $\beta_7$ using a classic LASSO, as $X_7$ is in $G_U$. Nevertheless, other variables are progressively added to $G_K$ when the prior knowledge level increases. We can see that the estimation based on the semi-LASSO is better than the one made with the 0-1 weighted LASSO. This difference certainly follows from the fact that the optimization performed on a smaller space in the semi-LASSO than the one from the 0-1 weighted LASSO, performed on $\mathbb{R}^{p+1}$. After we reach $70\%$ of prior knowledge, $X_7$ belongs to $G_K$, which means it has a $0$ weight in the 0-1 weighted LASSO, and is estimated via ordinary least squares estimator in semi-LASSO. At this point, both methods perform equally well for the estimation of $\beta_7$, since we force the variable to be part of the model.   </span>
<span id="cb25-554"><a href="#cb25-554" aria-hidden="true" tabindex="-1"></a>One can notice that for some levels of prior knowledge ($20\%$ to $40\%$), <span class="in">`glmnet`</span> often does not include the variable into the model, i.e. the estimated parameter $\widehat{\beta_7}$ is equal to zero, and thus generates some false negatives which impacts the sensitivity on @fig-sensi. This is not the case for the semi-LASSO method.</span>
<span id="cb25-555"><a href="#cb25-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-556"><a href="#cb25-556" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb25-557"><a href="#cb25-557" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-beta-esti</span></span>
<span id="cb25-558"><a href="#cb25-558" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Boxplots of the estimated values for a coefficient of the model, for each prior knowledge scenario and each method of parameters estimation."</span></span>
<span id="cb25-559"><a href="#cb25-559" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb25-560"><a href="#cb25-560" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(Data_final, aes(x = PriorKnowledge, y = V7)) + </span></span>
<span id="cb25-561"><a href="#cb25-561" aria-hidden="true" tabindex="-1"></a><span class="in">geom_boxplot(aes(fill = Method), position = position_dodge(0.9)) +</span></span>
<span id="cb25-562"><a href="#cb25-562" aria-hidden="true" tabindex="-1"></a><span class="in">scale_fill_manual(values = c("#00AFBB", "#E7B800")) +</span></span>
<span id="cb25-563"><a href="#cb25-563" aria-hidden="true" tabindex="-1"></a><span class="in">geom_hline(aes(yintercept = 7), color = "red")    +</span></span>
<span id="cb25-564"><a href="#cb25-564" aria-hidden="true" tabindex="-1"></a><span class="in">geom_vline(aes(xintercept = 8), color = "red") +</span></span>
<span id="cb25-565"><a href="#cb25-565" aria-hidden="true" tabindex="-1"></a><span class="in">labs(x = "% of prior knowledge", y = "X7")</span></span>
<span id="cb25-566"><a href="#cb25-566" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-567"><a href="#cb25-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-568"><a href="#cb25-568" aria-hidden="true" tabindex="-1"></a><span class="fu">## Other tests and some possible extensions</span></span>
<span id="cb25-569"><a href="#cb25-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-570"><a href="#cb25-570" aria-hidden="true" tabindex="-1"></a>Similar results as the ones presented in Section <span class="co">[</span><span class="ot">5</span><span class="co">](#results)</span> were obtained with different settings including :</span>
<span id="cb25-571"><a href="#cb25-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-572"><a href="#cb25-572" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The number of observations $n$ being set to $1000$ to have $p&lt;n$. </span>
<span id="cb25-573"><a href="#cb25-573" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Independent variables $(X_j, \, j \in <span class="co">[</span><span class="ot">1,p</span><span class="co">]</span>)$ instead of correlated ones.</span>
<span id="cb25-574"><a href="#cb25-574" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>A model without intercept^<span class="co">[</span><span class="ot"> During the redaction of this paper, we tested several options in `glmnet`, in particular the `intercept` parameter that we set to FALSE. It seems that when the data given in entry of the function is not centered beforehands, `glmnet` does not produce convincing results. We then found ourselves with significantly better results using the semi-LASSO algorithm. It appears that this [issue](https://stackoverflow.com/questions/49495494/glmnet-is-different-with-intercept-true-compared-to-intercept-false-and-with-pen) was raised years ago, but do not seem to be fixed. </span><span class="co">]</span>.</span>
<span id="cb25-575"><a href="#cb25-575" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Non-centered variables $(X_j, \, j \in <span class="co">[</span><span class="ot">1,p</span><span class="co">]</span>)$.</span>
<span id="cb25-576"><a href="#cb25-576" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Different values for $\sigma$.</span>
<span id="cb25-577"><a href="#cb25-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-578"><a href="#cb25-578" aria-hidden="true" tabindex="-1"></a>In all cases, the obtained graphs show similar behaviors as the ones presented here and are thus not included in the paper.   </span>
<span id="cb25-579"><a href="#cb25-579" aria-hidden="true" tabindex="-1"></a>Several extensions are possible using the framework of the semi-LASSO method. We can think at first of a weighted elasticnet method <span class="co">[</span><span class="ot">@zou2005regularization</span><span class="co">]</span>, with $0$ and $1$ weights. The mathematical formulation presented in Section <span class="co">[</span><span class="ot">3</span><span class="co">](#semilasso)</span> can indeed be generalized to this type of penalization.   </span>
<span id="cb25-580"><a href="#cb25-580" aria-hidden="true" tabindex="-1"></a>Another possible extension would be a classic weighted LASSO with some $0$ weights and other weights not all equal to $1$. In this case, we could again use the decomposition of Section <span class="co">[</span><span class="ot">3</span><span class="co">](#semilasso)</span>, but the classical LASSO in the semi-LASSO algorithm would be replaced by a weighted LASSO.</span>
<span id="cb25-581"><a href="#cb25-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-582"><a href="#cb25-582" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion</span></span>
<span id="cb25-583"><a href="#cb25-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-584"><a href="#cb25-584" aria-hidden="true" tabindex="-1"></a>This paper introduces a new weighted LASSO method, called semi-LASSO, designed for the integration of prior knowledge into the model. It relies on the <span class="in">`R`</span> package <span class="in">`glmnet`</span>, but does not use the <span class="in">`penalty.factor`</span> allowing to specify weights. Instead of that, it first transforms the problem to divide the optimization procedure into two steps. The first one is an ordinary least square method, which allows to reduce the space dimension for the second one: a classic LASSO procedure. The reduction of dimension in the semi-LASSO procedure gives significantly better results than the use of <span class="in">`penalty.factor`</span> implemented in <span class="in">`glmnet`</span> both on the proportion of true regressors detected by the method and on the prediction error. We can also see a better coefficient estimation with the semi-LASSO. These improvements come with a size restriction: our method can only be applied if $|G_K|\leq n$ because the first step is an ordinary least squares problem.   </span>
<span id="cb25-585"><a href="#cb25-585" aria-hidden="true" tabindex="-1"></a>Moreover, in terms of model selection, increasing the true positive rate using the semi-LASSO means decreasing the true negative rate, compared to the 0-1 weighted LASSO. Depending on the problem the user tries to solve, it is left to his discretion to use either semi-LASSO or <span class="in">`glmnet`</span> to reduce the most important rate at his eyes. The semi-LASSO procedure can also be applied in some extensions like weighted LASSO with different weights, or an elastic-net penalization.</span>
<span id="cb25-586"><a href="#cb25-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-587"><a href="#cb25-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-588"><a href="#cb25-588" aria-hidden="true" tabindex="-1"></a><span class="fu"># Acknowlegments { .unnumbered}</span></span>
<span id="cb25-589"><a href="#cb25-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-590"><a href="#cb25-590" aria-hidden="true" tabindex="-1"></a>The authors thank the Région Grand Est, Project LOR-IA THESE  for funding the PhD thesis of Anouk Rago.</span>
<span id="cb25-591"><a href="#cb25-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-592"><a href="#cb25-592" aria-hidden="true" tabindex="-1"></a><span class="fu"># Session information { .unnumbered}</span></span>
<span id="cb25-593"><a href="#cb25-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-594"><a href="#cb25-594" aria-hidden="true" tabindex="-1"></a><span class="in">```{r session-info, echo = TRUE}</span></span>
<span id="cb25-595"><a href="#cb25-595" aria-hidden="true" tabindex="-1"></a><span class="in">sessionInfo()</span></span>
<span id="cb25-596"><a href="#cb25-596" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-597"><a href="#cb25-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-598"><a href="#cb25-598" aria-hidden="true" tabindex="-1"></a><span class="fu"># Bibliography {.unnumbered}</span></span>
<span id="cb25-599"><a href="#cb25-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-600"><a href="#cb25-600" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb25-601"><a href="#cb25-601" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>